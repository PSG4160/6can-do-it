{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82d0d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166731e8-4b10-4968-838d-329473357328</td>\n",
       "      <td>Sylviah Chichi</td>\n",
       "      <td>Great App on the move ..... I can watch my mov...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "      <td>2024-10-21 14:15:31</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68bab7d0-2afc-4454-970f-159ced93d751</td>\n",
       "      <td>Marilyn Goeda</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-10-21 14:08:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6c2d3e85-b5ca-4228-93bd-abd2236eab51</td>\n",
       "      <td>Nikhil Pk</td>\n",
       "      <td>Need to improve and to update some error durin...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.131.0 build 3 50829</td>\n",
       "      <td>2024-10-21 13:54:11</td>\n",
       "      <td>8.131.0 build 3 50829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d62b0303-4c87-4c96-9c2c-a3ca6e0b056d</td>\n",
       "      <td>Mmesoma Eberechukwu</td>\n",
       "      <td>Netflix is a nice app,but not all the movies a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "      <td>2024-10-21 13:30:42</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6014252-863e-4e06-b440-25e0ece47a31</td>\n",
       "      <td>Keabetswe Monaise</td>\n",
       "      <td>Not much availability considering options on w...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "      <td>2024-10-21 13:28:43</td>\n",
       "      <td>8.136.0 build 3 50908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117129</th>\n",
       "      <td>a760ead9-e7aa-4ed1-a651-5c37c3600dac</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>i really like it! there are so many movies and...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-03 15:06:03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117130</th>\n",
       "      <td>4957f9e7-d7f4-4a52-9764-031cebcac83f</td>\n",
       "      <td>Captain Jeoy</td>\n",
       "      <td>I love Netflix. I always enjoy my time using it.</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8.34.0 build 4 50250</td>\n",
       "      <td>2022-08-15 16:16:30</td>\n",
       "      <td>8.34.0 build 4 50250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117131</th>\n",
       "      <td>9acf7586-7abf-4b50-8c50-3ede3b2a42c4</td>\n",
       "      <td>Suryansh</td>\n",
       "      <td>Sound quality is very slow of movies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-17 07:26:58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117132</th>\n",
       "      <td>32870f7f-c461-4256-b602-75244ca60248</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Rate is very expensive.. bcos we see netflix s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.17.0 build 13 34346</td>\n",
       "      <td>2019-07-21 09:41:42</td>\n",
       "      <td>7.17.0 build 13 34346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117133</th>\n",
       "      <td>dc1352e9-10a8-41ca-ab23-05d045b08e90</td>\n",
       "      <td>suraj soni</td>\n",
       "      <td>this app is awesome for english movies ,series...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-24 11:04:08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117134 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    reviewId             userName  \\\n",
       "0       166731e8-4b10-4968-838d-329473357328       Sylviah Chichi   \n",
       "1       68bab7d0-2afc-4454-970f-159ced93d751        Marilyn Goeda   \n",
       "2       6c2d3e85-b5ca-4228-93bd-abd2236eab51            Nikhil Pk   \n",
       "3       d62b0303-4c87-4c96-9c2c-a3ca6e0b056d  Mmesoma Eberechukwu   \n",
       "4       d6014252-863e-4e06-b440-25e0ece47a31    Keabetswe Monaise   \n",
       "...                                      ...                  ...   \n",
       "117129  a760ead9-e7aa-4ed1-a651-5c37c3600dac        A Google user   \n",
       "117130  4957f9e7-d7f4-4a52-9764-031cebcac83f         Captain Jeoy   \n",
       "117131  9acf7586-7abf-4b50-8c50-3ede3b2a42c4             Suryansh   \n",
       "117132  32870f7f-c461-4256-b602-75244ca60248        A Google user   \n",
       "117133  dc1352e9-10a8-41ca-ab23-05d045b08e90           suraj soni   \n",
       "\n",
       "                                                  content  score  \\\n",
       "0       Great App on the move ..... I can watch my mov...      5   \n",
       "1                                                    good      5   \n",
       "2       Need to improve and to update some error durin...      3   \n",
       "3       Netflix is a nice app,but not all the movies a...      3   \n",
       "4       Not much availability considering options on w...      3   \n",
       "...                                                   ...    ...   \n",
       "117129  i really like it! there are so many movies and...      5   \n",
       "117130   I love Netflix. I always enjoy my time using it.      5   \n",
       "117131               Sound quality is very slow of movies      1   \n",
       "117132  Rate is very expensive.. bcos we see netflix s...      1   \n",
       "117133  this app is awesome for english movies ,series...      4   \n",
       "\n",
       "        thumbsUpCount   reviewCreatedVersion                   at  \\\n",
       "0                   0  8.136.0 build 3 50908  2024-10-21 14:15:31   \n",
       "1                   0                    NaN  2024-10-21 14:08:15   \n",
       "2                   0  8.131.0 build 3 50829  2024-10-21 13:54:11   \n",
       "3                   0  8.136.0 build 3 50908  2024-10-21 13:30:42   \n",
       "4                   0  8.136.0 build 3 50908  2024-10-21 13:28:43   \n",
       "...               ...                    ...                  ...   \n",
       "117129              0                    NaN  2019-08-03 15:06:03   \n",
       "117130              0   8.34.0 build 4 50250  2022-08-15 16:16:30   \n",
       "117131              0                    NaN  2020-08-17 07:26:58   \n",
       "117132              0  7.17.0 build 13 34346  2019-07-21 09:41:42   \n",
       "117133              0                    NaN  2020-05-24 11:04:08   \n",
       "\n",
       "                   appVersion  \n",
       "0       8.136.0 build 3 50908  \n",
       "1                         NaN  \n",
       "2       8.131.0 build 3 50829  \n",
       "3       8.136.0 build 3 50908  \n",
       "4       8.136.0 build 3 50908  \n",
       "...                       ...  \n",
       "117129                    NaN  \n",
       "117130   8.34.0 build 4 50250  \n",
       "117131                    NaN  \n",
       "117132  7.17.0 build 13 34346  \n",
       "117133                    NaN  \n",
       "\n",
       "[117134 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = pd.read_csv(\"netflix_reviews.csv\")  # 파일 불러오기\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65088fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117134, 8)\n",
      "Index(['reviewId', 'userName', 'content', 'score', 'thumbsUpCount',\n",
      "       'reviewCreatedVersion', 'at', 'appVersion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape) \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7cb3df-259c-44a4-b30e-cb690ea9e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c2bffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166731e8-4b10-4968-838d-329473357328</td>\n",
       "      <td>Sylviah Chichi</td>\n",
       "      <td>Great App on the move ..... I can watch my mov...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68bab7d0-2afc-4454-970f-159ced93d751</td>\n",
       "      <td>Marilyn Goeda</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6c2d3e85-b5ca-4228-93bd-abd2236eab51</td>\n",
       "      <td>Nikhil Pk</td>\n",
       "      <td>Need to improve and to update some error durin...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d62b0303-4c87-4c96-9c2c-a3ca6e0b056d</td>\n",
       "      <td>Mmesoma Eberechukwu</td>\n",
       "      <td>Netflix is a nice app,but not all the movies a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d6014252-863e-4e06-b440-25e0ece47a31</td>\n",
       "      <td>Keabetswe Monaise</td>\n",
       "      <td>Not much availability considering options on w...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId             userName  \\\n",
       "0  166731e8-4b10-4968-838d-329473357328       Sylviah Chichi   \n",
       "1  68bab7d0-2afc-4454-970f-159ced93d751        Marilyn Goeda   \n",
       "2  6c2d3e85-b5ca-4228-93bd-abd2236eab51            Nikhil Pk   \n",
       "3  d62b0303-4c87-4c96-9c2c-a3ca6e0b056d  Mmesoma Eberechukwu   \n",
       "4  d6014252-863e-4e06-b440-25e0ece47a31    Keabetswe Monaise   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \n",
       "0  Great App on the move ..... I can watch my mov...      5              0  \n",
       "1                                               good      5              0  \n",
       "2  Need to improve and to update some error durin...      3              0  \n",
       "3  Netflix is a nice app,but not all the movies a...      3              0  \n",
       "4  Not much availability considering options on w...      3              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:,0:5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09104b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117129</th>\n",
       "      <td>a760ead9-e7aa-4ed1-a651-5c37c3600dac</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>i really like it! there are so many movies and...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117130</th>\n",
       "      <td>4957f9e7-d7f4-4a52-9764-031cebcac83f</td>\n",
       "      <td>Captain Jeoy</td>\n",
       "      <td>I love Netflix. I always enjoy my time using it.</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117131</th>\n",
       "      <td>9acf7586-7abf-4b50-8c50-3ede3b2a42c4</td>\n",
       "      <td>Suryansh</td>\n",
       "      <td>Sound quality is very slow of movies</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117132</th>\n",
       "      <td>32870f7f-c461-4256-b602-75244ca60248</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Rate is very expensive.. bcos we see netflix s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117133</th>\n",
       "      <td>dc1352e9-10a8-41ca-ab23-05d045b08e90</td>\n",
       "      <td>suraj soni</td>\n",
       "      <td>this app is awesome for english movies ,series...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    reviewId       userName  \\\n",
       "117129  a760ead9-e7aa-4ed1-a651-5c37c3600dac  A Google user   \n",
       "117130  4957f9e7-d7f4-4a52-9764-031cebcac83f   Captain Jeoy   \n",
       "117131  9acf7586-7abf-4b50-8c50-3ede3b2a42c4       Suryansh   \n",
       "117132  32870f7f-c461-4256-b602-75244ca60248  A Google user   \n",
       "117133  dc1352e9-10a8-41ca-ab23-05d045b08e90     suraj soni   \n",
       "\n",
       "                                                  content  score  \\\n",
       "117129  i really like it! there are so many movies and...      5   \n",
       "117130   I love Netflix. I always enjoy my time using it.      5   \n",
       "117131               Sound quality is very slow of movies      1   \n",
       "117132  Rate is very expensive.. bcos we see netflix s...      1   \n",
       "117133  this app is awesome for english movies ,series...      4   \n",
       "\n",
       "        thumbsUpCount  \n",
       "117129              0  \n",
       "117130              0  \n",
       "117131              0  \n",
       "117132              0  \n",
       "117133              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0234ed",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f61aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "import re\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    text = text.lower()  # 대문자를 소문자로\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 구두점 제거\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    text = text.strip()  # 띄어쓰기 제외하고 빈 칸 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dddb924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               reviewId             userName  \\\n",
      "0                   ebd       sylviah chichi   \n",
      "1          babdafcfcedd        marilyn goeda   \n",
      "2        cdebcabdabdeab            nikhil pk   \n",
      "3          dbccccacaebd  mmesoma eberechukwu   \n",
      "4             deebeecea    keabetswe monaise   \n",
      "...                 ...                  ...   \n",
      "117129  aeadeaaedaccdac        a google user   \n",
      "117130     fedfacebcacf         captain jeoy   \n",
      "117131   acfabfbcedebac             suryansh   \n",
      "117132           ffcbca        a google user   \n",
      "117133      dceacaabdbe           suraj soni   \n",
      "\n",
      "                                                  content  score  \\\n",
      "0       great app on the move  i can watch my movies a...      5   \n",
      "1                                                    good      5   \n",
      "2       need to improve and to update some error durin...      3   \n",
      "3       netflix is a nice appbut not all the movies ar...      3   \n",
      "4       not much availability considering options on w...      3   \n",
      "...                                                   ...    ...   \n",
      "117129  i really like it there are so many movies and ...      5   \n",
      "117130     i love netflix i always enjoy my time using it      5   \n",
      "117131               sound quality is very slow of movies      1   \n",
      "117132  rate is very expensive bcos we see netflix sun...      1   \n",
      "117133  this app is awesome for english movies series ...      4   \n",
      "\n",
      "        thumbsUpCount  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "...               ...  \n",
      "117129              0  \n",
      "117130              0  \n",
      "117131              0  \n",
      "117132              0  \n",
      "117133              0  \n",
      "\n",
      "[117134 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df['reviewId'] = df['reviewId'].apply(preprocess_text)\n",
    "df['userName'] = df['userName'].apply(preprocess_text)\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff748f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns  # 그래프를 그리기 위한 seaborn 라이브러리 임포트 (없으면 설치 바랍니다)\n",
    "import matplotlib.pyplot as plt  # 그래프 표시를 위한 pyplot 임포트\n",
    "\n",
    "score_counts = df['score'].value_counts().reset_index() # score 컬럼의 값에 대한 빈도수를 계산한 후 시각화\n",
    "score_counts.columns = ['score','score_count']\n",
    "# score_counts\n",
    "\n",
    "sns.barplot(x='score', y='score_count', data=score_counts, palette='Set1', hue = 'score', legend=False)\n",
    "# # sns.countplot(x=df['score']) : countplot 이용한 방법 , 간단하게 표현가능\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d1ad272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "12.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.__version__)  \n",
    "print(torch.version.cuda)  \n",
    "print(torch.cuda.is_available()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d35f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668b6de9-1ece-4737-9e1f-847509f4e7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117134lines [00:01, 82778.76lines/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "reviews = df['content'].tolist()  # 'content'를 리스트로 변환\n",
    "ratings = df['score'].tolist()    # 'score'를 리스트로 변환\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, text_pipeline, label_pipeline):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.text_pipeline(self.reviews[idx])\n",
    "        rating = self.label_pipeline(self.ratings[idx])\n",
    "        return torch.tensor(review), torch.tensor(rating)\n",
    "\n",
    "# 토크나이저 정의 (기본 영어 토크나이저)\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 어휘 사전 생성 함수\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# 어휘 사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(reviews))\n",
    "\n",
    "# 텍스트 파이프라인 정의 (어휘 사전에 있는 단어만 처리)\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text) if token in vocab]\n",
    "\n",
    "# 이미 score가 숫자형이라 label_pipeline 생략 가능\n",
    "def label_pipeline(label):\n",
    "    return label  # 평점이 이미 숫자형이라 변환 생략 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 데이터셋 정의\n",
    "train_dataset = ReviewDataset(train_reviews, train_ratings, text_pipeline, label_pipeline)\n",
    "test_dataset = ReviewDataset(test_reviews, test_ratings, text_pipeline, label_pipeline)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    reviews, ratings = zip(*batch)\n",
    "    reviews = pad_sequence([torch.tensor(r) for r in reviews], batch_first=True)\n",
    "    ratings = torch.tensor(ratings)\n",
    "    return reviews, ratings\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76b1fbb9-d285-4dc4-97ea-c7aa4d35232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43d971-55a8-4a79-a436-c50e52e0c3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd61bd25-9794-4cc9-afdc-c1c8f556084a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 모델 학습 실행\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     35\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# 에포크마다 손실을 추적\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (reviews, ratings) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     37\u001b[0m         reviews, ratings \u001b[38;5;241m=\u001b[39m reviews\u001b[38;5;241m.\u001b[39mto(device), ratings\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 데이터를 GPU로 이동\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mReviewDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 16\u001b[0m     review \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreviews\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pipeline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings[idx])\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(review), torch\u001b[38;5;241m.\u001b[39mtensor(rating)\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36mtext_pipeline\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_pipeline\u001b[39m(text):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenizer(text) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m vocab]\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_pipeline\u001b[39m(text):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenizer(text) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\torchtext\\vocab.py:109\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi\u001b[38;5;241m.\u001b[39mget(\u001b[43mVocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUNK\u001b[49m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 16\n",
    "HIDDEN_DIM = 32\n",
    "OUTPUT_DIM = len(set(ratings))  # 예측할 점수 개수\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 모델을 CUDA로 이동 (가능한 경우)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 에포크마다 손실을 추적\n",
    "        for i, (reviews, ratings) in enumerate(train_dataloader):\n",
    "            reviews, ratings = reviews.to(device), ratings.to(device)  # 데이터를 GPU로 이동\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(reviews)  # 모델에 입력하여 예측값 계산\n",
    "            loss = criterion(outputs, ratings)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 배치마다 손실 출력\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss/len(train_dataloader):.4f}')\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e34e4b-dda9-4d63-8fe0-e0c373029eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117134lines [00:01, 81889.02lines/s] \n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_17888\\2771336819.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 10/1465, Loss: 1.3810\n",
      "Epoch 1/10, Batch 20/1465, Loss: 1.4653\n",
      "Epoch 1/10, Batch 30/1465, Loss: 1.4734\n",
      "Epoch 1/10, Batch 40/1465, Loss: 1.5264\n",
      "Epoch 1/10, Batch 50/1465, Loss: 1.3799\n",
      "Epoch 1/10, Batch 60/1465, Loss: 1.4198\n",
      "Epoch 1/10, Batch 70/1465, Loss: 1.3761\n",
      "Epoch 1/10, Batch 80/1465, Loss: 1.3432\n",
      "Epoch 1/10, Batch 90/1465, Loss: 1.3956\n",
      "Epoch 1/10, Batch 100/1465, Loss: 1.5167\n",
      "Epoch 1/10, Batch 110/1465, Loss: 1.4003\n",
      "Epoch 1/10, Batch 120/1465, Loss: 1.3788\n",
      "Epoch 1/10, Batch 130/1465, Loss: 1.5288\n",
      "Epoch 1/10, Batch 140/1465, Loss: 1.4892\n",
      "Epoch 1/10, Batch 150/1465, Loss: 1.4253\n",
      "Epoch 1/10, Batch 160/1465, Loss: 1.3844\n",
      "Epoch 1/10, Batch 170/1465, Loss: 1.3808\n",
      "Epoch 1/10, Batch 180/1465, Loss: 1.3811\n",
      "Epoch 1/10, Batch 190/1465, Loss: 1.4670\n",
      "Epoch 1/10, Batch 200/1465, Loss: 1.3674\n",
      "Epoch 1/10, Batch 210/1465, Loss: 1.5136\n",
      "Epoch 1/10, Batch 220/1465, Loss: 1.3654\n",
      "Epoch 1/10, Batch 230/1465, Loss: 1.4793\n",
      "Epoch 1/10, Batch 240/1465, Loss: 1.3452\n",
      "Epoch 1/10, Batch 250/1465, Loss: 1.4622\n",
      "Epoch 1/10, Batch 260/1465, Loss: 1.4842\n",
      "Epoch 1/10, Batch 270/1465, Loss: 1.3593\n",
      "Epoch 1/10, Batch 280/1465, Loss: 1.5042\n",
      "Epoch 1/10, Batch 290/1465, Loss: 1.5091\n",
      "Epoch 1/10, Batch 300/1465, Loss: 1.5285\n",
      "Epoch 1/10, Batch 310/1465, Loss: 1.5512\n",
      "Epoch 1/10, Batch 320/1465, Loss: 1.4904\n",
      "Epoch 1/10, Batch 330/1465, Loss: 1.4532\n",
      "Epoch 1/10, Batch 340/1465, Loss: 1.3830\n",
      "Epoch 1/10, Batch 350/1465, Loss: 1.4475\n",
      "Epoch 1/10, Batch 360/1465, Loss: 1.2824\n",
      "Epoch 1/10, Batch 370/1465, Loss: 1.4613\n",
      "Epoch 1/10, Batch 380/1465, Loss: 1.3504\n",
      "Epoch 1/10, Batch 390/1465, Loss: 1.4273\n",
      "Epoch 1/10, Batch 400/1465, Loss: 1.4414\n",
      "Epoch 1/10, Batch 410/1465, Loss: 1.4183\n",
      "Epoch 1/10, Batch 420/1465, Loss: 1.2914\n",
      "Epoch 1/10, Batch 430/1465, Loss: 1.5809\n",
      "Epoch 1/10, Batch 440/1465, Loss: 1.4611\n",
      "Epoch 1/10, Batch 450/1465, Loss: 1.5313\n",
      "Epoch 1/10, Batch 460/1465, Loss: 1.4782\n",
      "Epoch 1/10, Batch 470/1465, Loss: 1.6143\n",
      "Epoch 1/10, Batch 480/1465, Loss: 1.3971\n",
      "Epoch 1/10, Batch 490/1465, Loss: 1.3154\n",
      "Epoch 1/10, Batch 500/1465, Loss: 1.3541\n",
      "Epoch 1/10, Batch 510/1465, Loss: 1.5000\n",
      "Epoch 1/10, Batch 520/1465, Loss: 1.3693\n",
      "Epoch 1/10, Batch 530/1465, Loss: 1.4867\n",
      "Epoch 1/10, Batch 540/1465, Loss: 1.4854\n",
      "Epoch 1/10, Batch 550/1465, Loss: 1.4322\n",
      "Epoch 1/10, Batch 560/1465, Loss: 1.4653\n",
      "Epoch 1/10, Batch 570/1465, Loss: 1.5167\n",
      "Epoch 1/10, Batch 580/1465, Loss: 1.3499\n",
      "Epoch 1/10, Batch 590/1465, Loss: 1.4188\n",
      "Epoch 1/10, Batch 600/1465, Loss: 1.3961\n",
      "Epoch 1/10, Batch 610/1465, Loss: 1.5804\n",
      "Epoch 1/10, Batch 620/1465, Loss: 1.4212\n",
      "Epoch 1/10, Batch 630/1465, Loss: 1.4649\n",
      "Epoch 1/10, Batch 640/1465, Loss: 1.3894\n",
      "Epoch 1/10, Batch 650/1465, Loss: 1.3960\n",
      "Epoch 1/10, Batch 660/1465, Loss: 1.3326\n",
      "Epoch 1/10, Batch 670/1465, Loss: 1.4142\n",
      "Epoch 1/10, Batch 680/1465, Loss: 1.5204\n",
      "Epoch 1/10, Batch 690/1465, Loss: 1.3343\n",
      "Epoch 1/10, Batch 700/1465, Loss: 1.5466\n",
      "Epoch 1/10, Batch 710/1465, Loss: 1.5492\n",
      "Epoch 1/10, Batch 720/1465, Loss: 1.2855\n",
      "Epoch 1/10, Batch 730/1465, Loss: 1.4005\n",
      "Epoch 1/10, Batch 740/1465, Loss: 1.5661\n",
      "Epoch 1/10, Batch 750/1465, Loss: 1.4526\n",
      "Epoch 1/10, Batch 760/1465, Loss: 1.3485\n",
      "Epoch 1/10, Batch 770/1465, Loss: 1.4763\n",
      "Epoch 1/10, Batch 780/1465, Loss: 1.3453\n",
      "Epoch 1/10, Batch 790/1465, Loss: 1.5329\n",
      "Epoch 1/10, Batch 800/1465, Loss: 1.4338\n",
      "Epoch 1/10, Batch 810/1465, Loss: 1.5176\n",
      "Epoch 1/10, Batch 820/1465, Loss: 1.4846\n",
      "Epoch 1/10, Batch 830/1465, Loss: 1.3613\n",
      "Epoch 1/10, Batch 840/1465, Loss: 1.3874\n",
      "Epoch 1/10, Batch 850/1465, Loss: 1.4115\n",
      "Epoch 1/10, Batch 860/1465, Loss: 1.4714\n",
      "Epoch 1/10, Batch 870/1465, Loss: 1.5188\n",
      "Epoch 1/10, Batch 880/1465, Loss: 1.2438\n",
      "Epoch 1/10, Batch 890/1465, Loss: 1.4789\n",
      "Epoch 1/10, Batch 900/1465, Loss: 1.5065\n",
      "Epoch 1/10, Batch 910/1465, Loss: 1.4302\n",
      "Epoch 1/10, Batch 920/1465, Loss: 1.4947\n",
      "Epoch 1/10, Batch 930/1465, Loss: 1.3446\n",
      "Epoch 1/10, Batch 940/1465, Loss: 1.3524\n",
      "Epoch 1/10, Batch 950/1465, Loss: 1.3573\n",
      "Epoch 1/10, Batch 960/1465, Loss: 1.4663\n",
      "Epoch 1/10, Batch 970/1465, Loss: 1.3062\n",
      "Epoch 1/10, Batch 980/1465, Loss: 1.4410\n",
      "Epoch 1/10, Batch 990/1465, Loss: 1.5055\n",
      "Epoch 1/10, Batch 1000/1465, Loss: 1.3520\n",
      "Epoch 1/10, Batch 1010/1465, Loss: 1.4289\n",
      "Epoch 1/10, Batch 1020/1465, Loss: 1.2953\n",
      "Epoch 1/10, Batch 1030/1465, Loss: 1.3208\n",
      "Epoch 1/10, Batch 1040/1465, Loss: 1.4184\n",
      "Epoch 1/10, Batch 1050/1465, Loss: 1.3439\n",
      "Epoch 1/10, Batch 1060/1465, Loss: 1.3920\n",
      "Epoch 1/10, Batch 1070/1465, Loss: 1.5100\n",
      "Epoch 1/10, Batch 1080/1465, Loss: 1.4287\n",
      "Epoch 1/10, Batch 1090/1465, Loss: 1.3611\n",
      "Epoch 1/10, Batch 1100/1465, Loss: 1.4926\n",
      "Epoch 1/10, Batch 1110/1465, Loss: 1.3542\n",
      "Epoch 1/10, Batch 1120/1465, Loss: 1.4776\n",
      "Epoch 1/10, Batch 1130/1465, Loss: 1.3736\n",
      "Epoch 1/10, Batch 1140/1465, Loss: 1.4997\n",
      "Epoch 1/10, Batch 1150/1465, Loss: 1.4074\n",
      "Epoch 1/10, Batch 1160/1465, Loss: 1.4002\n",
      "Epoch 1/10, Batch 1170/1465, Loss: 1.5580\n",
      "Epoch 1/10, Batch 1180/1465, Loss: 1.5020\n",
      "Epoch 1/10, Batch 1190/1465, Loss: 1.4766\n",
      "Epoch 1/10, Batch 1200/1465, Loss: 1.3658\n",
      "Epoch 1/10, Batch 1210/1465, Loss: 1.3714\n",
      "Epoch 1/10, Batch 1220/1465, Loss: 1.4684\n",
      "Epoch 1/10, Batch 1230/1465, Loss: 1.3491\n",
      "Epoch 1/10, Batch 1240/1465, Loss: 1.3568\n",
      "Epoch 1/10, Batch 1250/1465, Loss: 1.5153\n",
      "Epoch 1/10, Batch 1260/1465, Loss: 1.4516\n",
      "Epoch 1/10, Batch 1270/1465, Loss: 1.4036\n",
      "Epoch 1/10, Batch 1280/1465, Loss: 1.4864\n",
      "Epoch 1/10, Batch 1290/1465, Loss: 1.4145\n",
      "Epoch 1/10, Batch 1300/1465, Loss: 1.3796\n",
      "Epoch 1/10, Batch 1310/1465, Loss: 1.5113\n",
      "Epoch 1/10, Batch 1320/1465, Loss: 1.3921\n",
      "Epoch 1/10, Batch 1330/1465, Loss: 1.4792\n",
      "Epoch 1/10, Batch 1340/1465, Loss: 1.4688\n",
      "Epoch 1/10, Batch 1350/1465, Loss: 1.4290\n",
      "Epoch 1/10, Batch 1360/1465, Loss: 1.2789\n",
      "Epoch 1/10, Batch 1370/1465, Loss: 1.4859\n",
      "Epoch 1/10, Batch 1380/1465, Loss: 1.4233\n",
      "Epoch 1/10, Batch 1390/1465, Loss: 1.5285\n",
      "Epoch 1/10, Batch 1400/1465, Loss: 1.4582\n",
      "Epoch 1/10, Batch 1410/1465, Loss: 1.4270\n",
      "Epoch 1/10, Batch 1420/1465, Loss: 1.3469\n",
      "Epoch 1/10, Batch 1430/1465, Loss: 1.4075\n",
      "Epoch 1/10, Batch 1440/1465, Loss: 1.3415\n",
      "Epoch 1/10, Batch 1450/1465, Loss: 1.4304\n",
      "Epoch 1/10, Batch 1460/1465, Loss: 1.3528\n",
      "Epoch [1/10], Average Loss: 1.4336\n",
      "Epoch 2/10, Batch 10/1465, Loss: 1.4729\n",
      "Epoch 2/10, Batch 20/1465, Loss: 1.4660\n",
      "Epoch 2/10, Batch 30/1465, Loss: 1.5531\n",
      "Epoch 2/10, Batch 40/1465, Loss: 1.5126\n",
      "Epoch 2/10, Batch 50/1465, Loss: 1.3830\n",
      "Epoch 2/10, Batch 60/1465, Loss: 1.2930\n",
      "Epoch 2/10, Batch 70/1465, Loss: 1.3520\n",
      "Epoch 2/10, Batch 80/1465, Loss: 1.3877\n",
      "Epoch 2/10, Batch 90/1465, Loss: 1.2908\n",
      "Epoch 2/10, Batch 100/1465, Loss: 1.3582\n",
      "Epoch 2/10, Batch 110/1465, Loss: 1.3161\n",
      "Epoch 2/10, Batch 120/1465, Loss: 1.3810\n",
      "Epoch 2/10, Batch 130/1465, Loss: 1.3634\n",
      "Epoch 2/10, Batch 140/1465, Loss: 1.3542\n",
      "Epoch 2/10, Batch 150/1465, Loss: 1.4888\n",
      "Epoch 2/10, Batch 160/1465, Loss: 1.3943\n",
      "Epoch 2/10, Batch 170/1465, Loss: 1.3384\n",
      "Epoch 2/10, Batch 180/1465, Loss: 1.3807\n",
      "Epoch 2/10, Batch 190/1465, Loss: 1.4249\n",
      "Epoch 2/10, Batch 200/1465, Loss: 1.4632\n",
      "Epoch 2/10, Batch 210/1465, Loss: 1.5095\n",
      "Epoch 2/10, Batch 220/1465, Loss: 1.4034\n",
      "Epoch 2/10, Batch 230/1465, Loss: 1.3815\n",
      "Epoch 2/10, Batch 240/1465, Loss: 1.4649\n",
      "Epoch 2/10, Batch 250/1465, Loss: 1.3924\n",
      "Epoch 2/10, Batch 260/1465, Loss: 1.3129\n",
      "Epoch 2/10, Batch 270/1465, Loss: 1.4135\n",
      "Epoch 2/10, Batch 280/1465, Loss: 1.3546\n",
      "Epoch 2/10, Batch 290/1465, Loss: 1.2765\n",
      "Epoch 2/10, Batch 300/1465, Loss: 1.4453\n",
      "Epoch 2/10, Batch 310/1465, Loss: 1.3640\n",
      "Epoch 2/10, Batch 320/1465, Loss: 1.3654\n",
      "Epoch 2/10, Batch 330/1465, Loss: 1.3225\n",
      "Epoch 2/10, Batch 340/1465, Loss: 1.3855\n",
      "Epoch 2/10, Batch 350/1465, Loss: 1.5383\n",
      "Epoch 2/10, Batch 360/1465, Loss: 1.3394\n",
      "Epoch 2/10, Batch 370/1465, Loss: 1.5539\n",
      "Epoch 2/10, Batch 380/1465, Loss: 1.4459\n",
      "Epoch 2/10, Batch 390/1465, Loss: 1.4164\n",
      "Epoch 2/10, Batch 400/1465, Loss: 1.3458\n",
      "Epoch 2/10, Batch 410/1465, Loss: 1.3895\n",
      "Epoch 2/10, Batch 420/1465, Loss: 1.5080\n",
      "Epoch 2/10, Batch 430/1465, Loss: 1.5376\n",
      "Epoch 2/10, Batch 440/1465, Loss: 1.2559\n",
      "Epoch 2/10, Batch 450/1465, Loss: 1.6028\n",
      "Epoch 2/10, Batch 460/1465, Loss: 1.4483\n",
      "Epoch 2/10, Batch 470/1465, Loss: 1.4586\n",
      "Epoch 2/10, Batch 480/1465, Loss: 1.5270\n",
      "Epoch 2/10, Batch 490/1465, Loss: 1.4052\n",
      "Epoch 2/10, Batch 500/1465, Loss: 1.3556\n",
      "Epoch 2/10, Batch 510/1465, Loss: 1.5567\n",
      "Epoch 2/10, Batch 520/1465, Loss: 1.3749\n",
      "Epoch 2/10, Batch 530/1465, Loss: 1.3415\n",
      "Epoch 2/10, Batch 540/1465, Loss: 1.4624\n",
      "Epoch 2/10, Batch 550/1465, Loss: 1.4802\n",
      "Epoch 2/10, Batch 560/1465, Loss: 1.3165\n",
      "Epoch 2/10, Batch 570/1465, Loss: 1.6018\n",
      "Epoch 2/10, Batch 580/1465, Loss: 1.4123\n",
      "Epoch 2/10, Batch 590/1465, Loss: 1.3346\n",
      "Epoch 2/10, Batch 600/1465, Loss: 1.4234\n",
      "Epoch 2/10, Batch 610/1465, Loss: 1.3287\n",
      "Epoch 2/10, Batch 620/1465, Loss: 1.3066\n",
      "Epoch 2/10, Batch 630/1465, Loss: 1.3126\n",
      "Epoch 2/10, Batch 640/1465, Loss: 1.4803\n",
      "Epoch 2/10, Batch 650/1465, Loss: 1.3665\n",
      "Epoch 2/10, Batch 660/1465, Loss: 1.5559\n",
      "Epoch 2/10, Batch 670/1465, Loss: 1.4490\n",
      "Epoch 2/10, Batch 680/1465, Loss: 1.4141\n",
      "Epoch 2/10, Batch 690/1465, Loss: 1.3675\n",
      "Epoch 2/10, Batch 700/1465, Loss: 1.3520\n",
      "Epoch 2/10, Batch 710/1465, Loss: 1.3623\n",
      "Epoch 2/10, Batch 720/1465, Loss: 1.2592\n",
      "Epoch 2/10, Batch 730/1465, Loss: 1.3415\n",
      "Epoch 2/10, Batch 740/1465, Loss: 1.4473\n",
      "Epoch 2/10, Batch 750/1465, Loss: 1.5238\n",
      "Epoch 2/10, Batch 760/1465, Loss: 1.5785\n",
      "Epoch 2/10, Batch 770/1465, Loss: 1.2999\n",
      "Epoch 2/10, Batch 780/1465, Loss: 1.4372\n",
      "Epoch 2/10, Batch 790/1465, Loss: 1.4113\n",
      "Epoch 2/10, Batch 800/1465, Loss: 1.2946\n",
      "Epoch 2/10, Batch 810/1465, Loss: 1.5405\n",
      "Epoch 2/10, Batch 820/1465, Loss: 1.3994\n",
      "Epoch 2/10, Batch 830/1465, Loss: 1.3802\n",
      "Epoch 2/10, Batch 840/1465, Loss: 1.5474\n",
      "Epoch 2/10, Batch 850/1465, Loss: 1.3984\n",
      "Epoch 2/10, Batch 860/1465, Loss: 1.3841\n",
      "Epoch 2/10, Batch 870/1465, Loss: 1.5006\n",
      "Epoch 2/10, Batch 880/1465, Loss: 1.5877\n",
      "Epoch 2/10, Batch 890/1465, Loss: 1.5013\n",
      "Epoch 2/10, Batch 900/1465, Loss: 1.3674\n",
      "Epoch 2/10, Batch 910/1465, Loss: 1.3391\n",
      "Epoch 2/10, Batch 920/1465, Loss: 1.3714\n",
      "Epoch 2/10, Batch 930/1465, Loss: 1.4244\n",
      "Epoch 2/10, Batch 940/1465, Loss: 1.2336\n",
      "Epoch 2/10, Batch 950/1465, Loss: 1.3567\n",
      "Epoch 2/10, Batch 960/1465, Loss: 1.4230\n",
      "Epoch 2/10, Batch 970/1465, Loss: 1.4884\n",
      "Epoch 2/10, Batch 980/1465, Loss: 1.5665\n",
      "Epoch 2/10, Batch 990/1465, Loss: 1.4494\n",
      "Epoch 2/10, Batch 1000/1465, Loss: 1.3495\n",
      "Epoch 2/10, Batch 1010/1465, Loss: 1.4049\n",
      "Epoch 2/10, Batch 1020/1465, Loss: 1.3571\n",
      "Epoch 2/10, Batch 1030/1465, Loss: 1.5145\n",
      "Epoch 2/10, Batch 1040/1465, Loss: 1.4580\n",
      "Epoch 2/10, Batch 1050/1465, Loss: 1.5223\n",
      "Epoch 2/10, Batch 1060/1465, Loss: 1.4489\n",
      "Epoch 2/10, Batch 1070/1465, Loss: 1.4199\n",
      "Epoch 2/10, Batch 1080/1465, Loss: 1.4901\n",
      "Epoch 2/10, Batch 1090/1465, Loss: 1.4527\n",
      "Epoch 2/10, Batch 1100/1465, Loss: 1.4220\n",
      "Epoch 2/10, Batch 1110/1465, Loss: 1.4234\n",
      "Epoch 2/10, Batch 1120/1465, Loss: 1.4047\n",
      "Epoch 2/10, Batch 1130/1465, Loss: 1.3652\n",
      "Epoch 2/10, Batch 1140/1465, Loss: 1.3668\n",
      "Epoch 2/10, Batch 1150/1465, Loss: 1.3895\n",
      "Epoch 2/10, Batch 1160/1465, Loss: 1.4752\n",
      "Epoch 2/10, Batch 1170/1465, Loss: 1.2690\n",
      "Epoch 2/10, Batch 1180/1465, Loss: 1.4876\n",
      "Epoch 2/10, Batch 1190/1465, Loss: 1.4826\n",
      "Epoch 2/10, Batch 1200/1465, Loss: 1.4929\n",
      "Epoch 2/10, Batch 1210/1465, Loss: 1.4073\n",
      "Epoch 2/10, Batch 1220/1465, Loss: 1.4407\n",
      "Epoch 2/10, Batch 1230/1465, Loss: 1.2707\n",
      "Epoch 2/10, Batch 1240/1465, Loss: 1.4914\n",
      "Epoch 2/10, Batch 1250/1465, Loss: 1.3667\n",
      "Epoch 2/10, Batch 1260/1465, Loss: 1.5596\n",
      "Epoch 2/10, Batch 1270/1465, Loss: 1.3924\n",
      "Epoch 2/10, Batch 1280/1465, Loss: 1.3622\n",
      "Epoch 2/10, Batch 1290/1465, Loss: 1.3383\n",
      "Epoch 2/10, Batch 1300/1465, Loss: 1.5742\n",
      "Epoch 2/10, Batch 1310/1465, Loss: 1.4552\n",
      "Epoch 2/10, Batch 1320/1465, Loss: 1.3638\n",
      "Epoch 2/10, Batch 1330/1465, Loss: 1.5370\n",
      "Epoch 2/10, Batch 1340/1465, Loss: 1.3089\n",
      "Epoch 2/10, Batch 1350/1465, Loss: 1.4436\n",
      "Epoch 2/10, Batch 1360/1465, Loss: 1.5015\n",
      "Epoch 2/10, Batch 1370/1465, Loss: 1.4719\n",
      "Epoch 2/10, Batch 1380/1465, Loss: 1.4301\n",
      "Epoch 2/10, Batch 1390/1465, Loss: 1.5496\n",
      "Epoch 2/10, Batch 1400/1465, Loss: 1.4474\n",
      "Epoch 2/10, Batch 1410/1465, Loss: 1.2325\n",
      "Epoch 2/10, Batch 1420/1465, Loss: 1.3915\n",
      "Epoch 2/10, Batch 1430/1465, Loss: 1.2341\n",
      "Epoch 2/10, Batch 1440/1465, Loss: 1.2769\n",
      "Epoch 2/10, Batch 1450/1465, Loss: 1.2886\n",
      "Epoch 2/10, Batch 1460/1465, Loss: 1.2633\n",
      "Epoch [2/10], Average Loss: 1.4221\n",
      "Epoch 3/10, Batch 10/1465, Loss: 1.2427\n",
      "Epoch 3/10, Batch 20/1465, Loss: 1.1654\n",
      "Epoch 3/10, Batch 30/1465, Loss: 1.3026\n",
      "Epoch 3/10, Batch 40/1465, Loss: 1.2246\n",
      "Epoch 3/10, Batch 50/1465, Loss: 1.3348\n",
      "Epoch 3/10, Batch 60/1465, Loss: 1.1064\n",
      "Epoch 3/10, Batch 70/1465, Loss: 1.3994\n",
      "Epoch 3/10, Batch 80/1465, Loss: 1.1811\n",
      "Epoch 3/10, Batch 90/1465, Loss: 1.1123\n",
      "Epoch 3/10, Batch 100/1465, Loss: 1.1354\n",
      "Epoch 3/10, Batch 110/1465, Loss: 1.2596\n",
      "Epoch 3/10, Batch 120/1465, Loss: 1.1634\n",
      "Epoch 3/10, Batch 130/1465, Loss: 1.6184\n",
      "Epoch 3/10, Batch 140/1465, Loss: 1.3184\n",
      "Epoch 3/10, Batch 150/1465, Loss: 1.1647\n",
      "Epoch 3/10, Batch 160/1465, Loss: 1.1161\n",
      "Epoch 3/10, Batch 170/1465, Loss: 1.0588\n",
      "Epoch 3/10, Batch 180/1465, Loss: 1.2255\n",
      "Epoch 3/10, Batch 190/1465, Loss: 1.2060\n",
      "Epoch 3/10, Batch 200/1465, Loss: 1.0031\n",
      "Epoch 3/10, Batch 210/1465, Loss: 1.3352\n",
      "Epoch 3/10, Batch 220/1465, Loss: 1.0892\n",
      "Epoch 3/10, Batch 230/1465, Loss: 1.1721\n",
      "Epoch 3/10, Batch 240/1465, Loss: 1.1076\n",
      "Epoch 3/10, Batch 250/1465, Loss: 1.1181\n",
      "Epoch 3/10, Batch 260/1465, Loss: 1.0369\n",
      "Epoch 3/10, Batch 270/1465, Loss: 1.0969\n",
      "Epoch 3/10, Batch 280/1465, Loss: 1.2221\n",
      "Epoch 3/10, Batch 290/1465, Loss: 1.3970\n",
      "Epoch 3/10, Batch 300/1465, Loss: 1.2392\n",
      "Epoch 3/10, Batch 310/1465, Loss: 1.2886\n",
      "Epoch 3/10, Batch 320/1465, Loss: 1.1729\n",
      "Epoch 3/10, Batch 330/1465, Loss: 1.0638\n",
      "Epoch 3/10, Batch 340/1465, Loss: 1.2592\n",
      "Epoch 3/10, Batch 350/1465, Loss: 1.3682\n",
      "Epoch 3/10, Batch 360/1465, Loss: 1.0552\n",
      "Epoch 3/10, Batch 370/1465, Loss: 1.2298\n",
      "Epoch 3/10, Batch 380/1465, Loss: 1.1667\n",
      "Epoch 3/10, Batch 390/1465, Loss: 1.0182\n",
      "Epoch 3/10, Batch 400/1465, Loss: 1.2191\n",
      "Epoch 3/10, Batch 410/1465, Loss: 1.2363\n",
      "Epoch 3/10, Batch 420/1465, Loss: 1.1293\n",
      "Epoch 3/10, Batch 430/1465, Loss: 1.1372\n",
      "Epoch 3/10, Batch 440/1465, Loss: 1.1240\n",
      "Epoch 3/10, Batch 450/1465, Loss: 1.0666\n",
      "Epoch 3/10, Batch 460/1465, Loss: 1.1173\n",
      "Epoch 3/10, Batch 470/1465, Loss: 1.2532\n",
      "Epoch 3/10, Batch 480/1465, Loss: 1.2390\n",
      "Epoch 3/10, Batch 490/1465, Loss: 1.0376\n",
      "Epoch 3/10, Batch 500/1465, Loss: 0.8576\n",
      "Epoch 3/10, Batch 510/1465, Loss: 1.2114\n",
      "Epoch 3/10, Batch 520/1465, Loss: 1.1349\n",
      "Epoch 3/10, Batch 530/1465, Loss: 1.0824\n",
      "Epoch 3/10, Batch 540/1465, Loss: 1.3309\n",
      "Epoch 3/10, Batch 550/1465, Loss: 0.9980\n",
      "Epoch 3/10, Batch 560/1465, Loss: 1.3553\n",
      "Epoch 3/10, Batch 570/1465, Loss: 1.2117\n",
      "Epoch 3/10, Batch 580/1465, Loss: 1.1518\n",
      "Epoch 3/10, Batch 590/1465, Loss: 0.9251\n",
      "Epoch 3/10, Batch 600/1465, Loss: 0.9229\n",
      "Epoch 3/10, Batch 610/1465, Loss: 1.0384\n",
      "Epoch 3/10, Batch 620/1465, Loss: 1.0995\n",
      "Epoch 3/10, Batch 630/1465, Loss: 0.9705\n",
      "Epoch 3/10, Batch 640/1465, Loss: 1.1193\n",
      "Epoch 3/10, Batch 650/1465, Loss: 1.2649\n",
      "Epoch 3/10, Batch 660/1465, Loss: 0.9917\n",
      "Epoch 3/10, Batch 670/1465, Loss: 1.1231\n",
      "Epoch 3/10, Batch 680/1465, Loss: 0.9756\n",
      "Epoch 3/10, Batch 690/1465, Loss: 1.2322\n",
      "Epoch 3/10, Batch 700/1465, Loss: 0.9636\n",
      "Epoch 3/10, Batch 710/1465, Loss: 1.1325\n",
      "Epoch 3/10, Batch 720/1465, Loss: 0.8216\n",
      "Epoch 3/10, Batch 730/1465, Loss: 1.1956\n",
      "Epoch 3/10, Batch 740/1465, Loss: 1.3281\n",
      "Epoch 3/10, Batch 750/1465, Loss: 1.0960\n",
      "Epoch 3/10, Batch 760/1465, Loss: 1.2824\n",
      "Epoch 3/10, Batch 770/1465, Loss: 1.0575\n",
      "Epoch 3/10, Batch 780/1465, Loss: 1.2470\n",
      "Epoch 3/10, Batch 790/1465, Loss: 1.0981\n",
      "Epoch 3/10, Batch 800/1465, Loss: 1.1343\n",
      "Epoch 3/10, Batch 810/1465, Loss: 1.0531\n",
      "Epoch 3/10, Batch 820/1465, Loss: 1.0183\n",
      "Epoch 3/10, Batch 830/1465, Loss: 1.1909\n",
      "Epoch 3/10, Batch 840/1465, Loss: 1.1678\n",
      "Epoch 3/10, Batch 850/1465, Loss: 1.1726\n",
      "Epoch 3/10, Batch 860/1465, Loss: 1.1789\n",
      "Epoch 3/10, Batch 870/1465, Loss: 1.1127\n",
      "Epoch 3/10, Batch 880/1465, Loss: 1.2086\n",
      "Epoch 3/10, Batch 890/1465, Loss: 1.2293\n",
      "Epoch 3/10, Batch 900/1465, Loss: 1.0945\n",
      "Epoch 3/10, Batch 910/1465, Loss: 0.9540\n",
      "Epoch 3/10, Batch 920/1465, Loss: 1.1555\n",
      "Epoch 3/10, Batch 930/1465, Loss: 1.0050\n",
      "Epoch 3/10, Batch 940/1465, Loss: 1.2217\n",
      "Epoch 3/10, Batch 950/1465, Loss: 1.0293\n",
      "Epoch 3/10, Batch 960/1465, Loss: 1.0550\n",
      "Epoch 3/10, Batch 970/1465, Loss: 1.0793\n",
      "Epoch 3/10, Batch 980/1465, Loss: 1.1557\n",
      "Epoch 3/10, Batch 990/1465, Loss: 0.9278\n",
      "Epoch 3/10, Batch 1000/1465, Loss: 1.2400\n",
      "Epoch 3/10, Batch 1010/1465, Loss: 1.1884\n",
      "Epoch 3/10, Batch 1020/1465, Loss: 0.9731\n",
      "Epoch 3/10, Batch 1030/1465, Loss: 1.0836\n",
      "Epoch 3/10, Batch 1040/1465, Loss: 1.2031\n",
      "Epoch 3/10, Batch 1050/1465, Loss: 1.1415\n",
      "Epoch 3/10, Batch 1060/1465, Loss: 1.0486\n",
      "Epoch 3/10, Batch 1070/1465, Loss: 1.0068\n",
      "Epoch 3/10, Batch 1080/1465, Loss: 1.0520\n",
      "Epoch 3/10, Batch 1090/1465, Loss: 1.1687\n",
      "Epoch 3/10, Batch 1100/1465, Loss: 1.2045\n",
      "Epoch 3/10, Batch 1110/1465, Loss: 1.1364\n",
      "Epoch 3/10, Batch 1120/1465, Loss: 0.9228\n",
      "Epoch 3/10, Batch 1130/1465, Loss: 0.9593\n",
      "Epoch 3/10, Batch 1140/1465, Loss: 0.9741\n",
      "Epoch 3/10, Batch 1150/1465, Loss: 1.1163\n",
      "Epoch 3/10, Batch 1160/1465, Loss: 1.1625\n",
      "Epoch 3/10, Batch 1170/1465, Loss: 1.0628\n",
      "Epoch 3/10, Batch 1180/1465, Loss: 1.1006\n",
      "Epoch 3/10, Batch 1190/1465, Loss: 1.1853\n",
      "Epoch 3/10, Batch 1200/1465, Loss: 0.8595\n",
      "Epoch 3/10, Batch 1210/1465, Loss: 1.0658\n",
      "Epoch 3/10, Batch 1220/1465, Loss: 1.2411\n",
      "Epoch 3/10, Batch 1230/1465, Loss: 1.1765\n",
      "Epoch 3/10, Batch 1240/1465, Loss: 0.9675\n",
      "Epoch 3/10, Batch 1250/1465, Loss: 1.3076\n",
      "Epoch 3/10, Batch 1260/1465, Loss: 1.0224\n",
      "Epoch 3/10, Batch 1270/1465, Loss: 1.2567\n",
      "Epoch 3/10, Batch 1280/1465, Loss: 1.1215\n",
      "Epoch 3/10, Batch 1290/1465, Loss: 1.2257\n",
      "Epoch 3/10, Batch 1300/1465, Loss: 1.0767\n",
      "Epoch 3/10, Batch 1310/1465, Loss: 1.0392\n",
      "Epoch 3/10, Batch 1320/1465, Loss: 1.1763\n",
      "Epoch 3/10, Batch 1330/1465, Loss: 1.1068\n",
      "Epoch 3/10, Batch 1340/1465, Loss: 1.0527\n",
      "Epoch 3/10, Batch 1350/1465, Loss: 0.9657\n",
      "Epoch 3/10, Batch 1360/1465, Loss: 1.0904\n",
      "Epoch 3/10, Batch 1370/1465, Loss: 1.1617\n",
      "Epoch 3/10, Batch 1380/1465, Loss: 1.0963\n",
      "Epoch 3/10, Batch 1390/1465, Loss: 0.9585\n",
      "Epoch 3/10, Batch 1400/1465, Loss: 0.9596\n",
      "Epoch 3/10, Batch 1410/1465, Loss: 0.8870\n",
      "Epoch 3/10, Batch 1420/1465, Loss: 1.2351\n",
      "Epoch 3/10, Batch 1430/1465, Loss: 1.0594\n",
      "Epoch 3/10, Batch 1440/1465, Loss: 1.1985\n",
      "Epoch 3/10, Batch 1450/1465, Loss: 1.1109\n",
      "Epoch 3/10, Batch 1460/1465, Loss: 1.2664\n",
      "Epoch [3/10], Average Loss: 1.1226\n",
      "Epoch 4/10, Batch 10/1465, Loss: 0.9905\n",
      "Epoch 4/10, Batch 20/1465, Loss: 0.8710\n",
      "Epoch 4/10, Batch 30/1465, Loss: 0.9957\n",
      "Epoch 4/10, Batch 40/1465, Loss: 0.9452\n",
      "Epoch 4/10, Batch 50/1465, Loss: 1.0236\n",
      "Epoch 4/10, Batch 60/1465, Loss: 0.9529\n",
      "Epoch 4/10, Batch 70/1465, Loss: 1.0861\n",
      "Epoch 4/10, Batch 80/1465, Loss: 1.0401\n",
      "Epoch 4/10, Batch 90/1465, Loss: 1.0795\n",
      "Epoch 4/10, Batch 100/1465, Loss: 1.0154\n",
      "Epoch 4/10, Batch 110/1465, Loss: 1.2385\n",
      "Epoch 4/10, Batch 120/1465, Loss: 0.8271\n",
      "Epoch 4/10, Batch 130/1465, Loss: 0.9588\n",
      "Epoch 4/10, Batch 140/1465, Loss: 0.9645\n",
      "Epoch 4/10, Batch 150/1465, Loss: 0.9090\n",
      "Epoch 4/10, Batch 160/1465, Loss: 0.8307\n",
      "Epoch 4/10, Batch 170/1465, Loss: 1.1221\n",
      "Epoch 4/10, Batch 180/1465, Loss: 1.1247\n",
      "Epoch 4/10, Batch 190/1465, Loss: 1.0455\n",
      "Epoch 4/10, Batch 200/1465, Loss: 0.9720\n",
      "Epoch 4/10, Batch 210/1465, Loss: 0.9919\n",
      "Epoch 4/10, Batch 220/1465, Loss: 0.9955\n",
      "Epoch 4/10, Batch 230/1465, Loss: 0.9441\n",
      "Epoch 4/10, Batch 240/1465, Loss: 1.1619\n",
      "Epoch 4/10, Batch 250/1465, Loss: 0.9914\n",
      "Epoch 4/10, Batch 260/1465, Loss: 1.1061\n",
      "Epoch 4/10, Batch 270/1465, Loss: 1.0073\n",
      "Epoch 4/10, Batch 280/1465, Loss: 0.8438\n",
      "Epoch 4/10, Batch 290/1465, Loss: 0.9647\n",
      "Epoch 4/10, Batch 300/1465, Loss: 1.0946\n",
      "Epoch 4/10, Batch 310/1465, Loss: 1.1656\n",
      "Epoch 4/10, Batch 320/1465, Loss: 0.9085\n",
      "Epoch 4/10, Batch 330/1465, Loss: 0.8903\n",
      "Epoch 4/10, Batch 340/1465, Loss: 1.1471\n",
      "Epoch 4/10, Batch 350/1465, Loss: 1.0918\n",
      "Epoch 4/10, Batch 360/1465, Loss: 0.9474\n",
      "Epoch 4/10, Batch 370/1465, Loss: 1.1552\n",
      "Epoch 4/10, Batch 380/1465, Loss: 1.2758\n",
      "Epoch 4/10, Batch 390/1465, Loss: 0.8687\n",
      "Epoch 4/10, Batch 400/1465, Loss: 1.1613\n",
      "Epoch 4/10, Batch 410/1465, Loss: 1.1029\n",
      "Epoch 4/10, Batch 420/1465, Loss: 0.9402\n",
      "Epoch 4/10, Batch 430/1465, Loss: 1.2798\n",
      "Epoch 4/10, Batch 440/1465, Loss: 1.2101\n",
      "Epoch 4/10, Batch 450/1465, Loss: 1.1925\n",
      "Epoch 4/10, Batch 460/1465, Loss: 0.8723\n",
      "Epoch 4/10, Batch 470/1465, Loss: 0.9457\n",
      "Epoch 4/10, Batch 480/1465, Loss: 0.8876\n",
      "Epoch 4/10, Batch 490/1465, Loss: 1.2572\n",
      "Epoch 4/10, Batch 500/1465, Loss: 1.0783\n",
      "Epoch 4/10, Batch 510/1465, Loss: 0.8344\n",
      "Epoch 4/10, Batch 520/1465, Loss: 1.0741\n",
      "Epoch 4/10, Batch 530/1465, Loss: 1.1261\n",
      "Epoch 4/10, Batch 540/1465, Loss: 0.9934\n",
      "Epoch 4/10, Batch 550/1465, Loss: 0.9905\n",
      "Epoch 4/10, Batch 560/1465, Loss: 0.9097\n",
      "Epoch 4/10, Batch 570/1465, Loss: 1.1147\n",
      "Epoch 4/10, Batch 580/1465, Loss: 1.1247\n",
      "Epoch 4/10, Batch 590/1465, Loss: 1.1214\n",
      "Epoch 4/10, Batch 600/1465, Loss: 1.2532\n",
      "Epoch 4/10, Batch 610/1465, Loss: 1.0136\n",
      "Epoch 4/10, Batch 620/1465, Loss: 0.8648\n",
      "Epoch 4/10, Batch 630/1465, Loss: 1.0107\n",
      "Epoch 4/10, Batch 640/1465, Loss: 1.1681\n",
      "Epoch 4/10, Batch 650/1465, Loss: 1.0109\n",
      "Epoch 4/10, Batch 660/1465, Loss: 0.8521\n",
      "Epoch 4/10, Batch 670/1465, Loss: 1.0655\n",
      "Epoch 4/10, Batch 680/1465, Loss: 0.9103\n",
      "Epoch 4/10, Batch 690/1465, Loss: 0.8467\n",
      "Epoch 4/10, Batch 700/1465, Loss: 1.1951\n",
      "Epoch 4/10, Batch 710/1465, Loss: 0.8326\n",
      "Epoch 4/10, Batch 720/1465, Loss: 1.1274\n",
      "Epoch 4/10, Batch 730/1465, Loss: 1.0669\n",
      "Epoch 4/10, Batch 740/1465, Loss: 0.9687\n",
      "Epoch 4/10, Batch 750/1465, Loss: 1.1174\n",
      "Epoch 4/10, Batch 760/1465, Loss: 1.0775\n",
      "Epoch 4/10, Batch 770/1465, Loss: 0.9953\n",
      "Epoch 4/10, Batch 780/1465, Loss: 0.9490\n",
      "Epoch 4/10, Batch 790/1465, Loss: 1.2148\n",
      "Epoch 4/10, Batch 800/1465, Loss: 0.9569\n",
      "Epoch 4/10, Batch 810/1465, Loss: 0.8609\n",
      "Epoch 4/10, Batch 820/1465, Loss: 0.9961\n",
      "Epoch 4/10, Batch 830/1465, Loss: 1.0643\n",
      "Epoch 4/10, Batch 840/1465, Loss: 1.1312\n",
      "Epoch 4/10, Batch 850/1465, Loss: 0.9115\n",
      "Epoch 4/10, Batch 860/1465, Loss: 1.1491\n",
      "Epoch 4/10, Batch 870/1465, Loss: 1.0603\n",
      "Epoch 4/10, Batch 880/1465, Loss: 1.0164\n",
      "Epoch 4/10, Batch 890/1465, Loss: 0.9357\n",
      "Epoch 4/10, Batch 900/1465, Loss: 1.1608\n",
      "Epoch 4/10, Batch 910/1465, Loss: 1.0930\n",
      "Epoch 4/10, Batch 920/1465, Loss: 1.0907\n",
      "Epoch 4/10, Batch 930/1465, Loss: 1.1360\n",
      "Epoch 4/10, Batch 940/1465, Loss: 0.9797\n",
      "Epoch 4/10, Batch 950/1465, Loss: 1.0439\n",
      "Epoch 4/10, Batch 960/1465, Loss: 0.9746\n",
      "Epoch 4/10, Batch 970/1465, Loss: 1.0501\n",
      "Epoch 4/10, Batch 980/1465, Loss: 1.0405\n",
      "Epoch 4/10, Batch 990/1465, Loss: 1.1876\n",
      "Epoch 4/10, Batch 1000/1465, Loss: 0.8857\n",
      "Epoch 4/10, Batch 1010/1465, Loss: 1.0028\n",
      "Epoch 4/10, Batch 1020/1465, Loss: 1.1672\n",
      "Epoch 4/10, Batch 1030/1465, Loss: 0.8638\n",
      "Epoch 4/10, Batch 1040/1465, Loss: 1.0348\n",
      "Epoch 4/10, Batch 1050/1465, Loss: 0.9033\n",
      "Epoch 4/10, Batch 1060/1465, Loss: 1.1130\n",
      "Epoch 4/10, Batch 1070/1465, Loss: 0.9529\n",
      "Epoch 4/10, Batch 1080/1465, Loss: 1.0965\n",
      "Epoch 4/10, Batch 1090/1465, Loss: 1.1457\n",
      "Epoch 4/10, Batch 1100/1465, Loss: 1.0882\n",
      "Epoch 4/10, Batch 1110/1465, Loss: 0.9849\n",
      "Epoch 4/10, Batch 1120/1465, Loss: 1.1759\n",
      "Epoch 4/10, Batch 1130/1465, Loss: 0.9926\n",
      "Epoch 4/10, Batch 1140/1465, Loss: 1.1986\n",
      "Epoch 4/10, Batch 1150/1465, Loss: 0.8860\n",
      "Epoch 4/10, Batch 1160/1465, Loss: 1.0446\n",
      "Epoch 4/10, Batch 1170/1465, Loss: 1.0608\n",
      "Epoch 4/10, Batch 1180/1465, Loss: 0.9038\n",
      "Epoch 4/10, Batch 1190/1465, Loss: 1.0981\n",
      "Epoch 4/10, Batch 1200/1465, Loss: 0.9053\n",
      "Epoch 4/10, Batch 1210/1465, Loss: 1.1242\n",
      "Epoch 4/10, Batch 1220/1465, Loss: 1.2368\n",
      "Epoch 4/10, Batch 1230/1465, Loss: 1.1935\n",
      "Epoch 4/10, Batch 1240/1465, Loss: 1.2179\n",
      "Epoch 4/10, Batch 1250/1465, Loss: 0.8098\n",
      "Epoch 4/10, Batch 1260/1465, Loss: 1.1443\n",
      "Epoch 4/10, Batch 1270/1465, Loss: 0.9641\n",
      "Epoch 4/10, Batch 1280/1465, Loss: 1.0424\n",
      "Epoch 4/10, Batch 1290/1465, Loss: 1.0384\n",
      "Epoch 4/10, Batch 1300/1465, Loss: 1.3755\n",
      "Epoch 4/10, Batch 1310/1465, Loss: 1.0034\n",
      "Epoch 4/10, Batch 1320/1465, Loss: 0.8576\n",
      "Epoch 4/10, Batch 1330/1465, Loss: 1.0774\n",
      "Epoch 4/10, Batch 1340/1465, Loss: 0.9535\n",
      "Epoch 4/10, Batch 1350/1465, Loss: 1.1007\n",
      "Epoch 4/10, Batch 1360/1465, Loss: 1.1185\n",
      "Epoch 4/10, Batch 1370/1465, Loss: 1.0430\n",
      "Epoch 4/10, Batch 1380/1465, Loss: 0.9676\n",
      "Epoch 4/10, Batch 1390/1465, Loss: 1.0460\n",
      "Epoch 4/10, Batch 1400/1465, Loss: 1.1622\n",
      "Epoch 4/10, Batch 1410/1465, Loss: 1.1569\n",
      "Epoch 4/10, Batch 1420/1465, Loss: 1.2637\n",
      "Epoch 4/10, Batch 1430/1465, Loss: 1.1564\n",
      "Epoch 4/10, Batch 1440/1465, Loss: 1.2212\n",
      "Epoch 4/10, Batch 1450/1465, Loss: 1.0437\n",
      "Epoch 4/10, Batch 1460/1465, Loss: 0.8714\n",
      "Epoch [4/10], Average Loss: 1.0449\n",
      "Epoch 5/10, Batch 10/1465, Loss: 1.0743\n",
      "Epoch 5/10, Batch 20/1465, Loss: 1.0886\n",
      "Epoch 5/10, Batch 30/1465, Loss: 1.0785\n",
      "Epoch 5/10, Batch 40/1465, Loss: 1.0148\n",
      "Epoch 5/10, Batch 50/1465, Loss: 0.8681\n",
      "Epoch 5/10, Batch 60/1465, Loss: 1.0270\n",
      "Epoch 5/10, Batch 70/1465, Loss: 0.8240\n",
      "Epoch 5/10, Batch 80/1465, Loss: 1.0864\n",
      "Epoch 5/10, Batch 90/1465, Loss: 1.0277\n",
      "Epoch 5/10, Batch 100/1465, Loss: 0.8912\n",
      "Epoch 5/10, Batch 110/1465, Loss: 0.9159\n",
      "Epoch 5/10, Batch 120/1465, Loss: 1.2854\n",
      "Epoch 5/10, Batch 130/1465, Loss: 1.0391\n",
      "Epoch 5/10, Batch 140/1465, Loss: 1.1447\n",
      "Epoch 5/10, Batch 150/1465, Loss: 1.0292\n",
      "Epoch 5/10, Batch 160/1465, Loss: 1.0054\n",
      "Epoch 5/10, Batch 170/1465, Loss: 1.0254\n",
      "Epoch 5/10, Batch 180/1465, Loss: 1.1904\n",
      "Epoch 5/10, Batch 190/1465, Loss: 1.0160\n",
      "Epoch 5/10, Batch 200/1465, Loss: 0.9875\n",
      "Epoch 5/10, Batch 210/1465, Loss: 0.8136\n",
      "Epoch 5/10, Batch 220/1465, Loss: 1.0968\n",
      "Epoch 5/10, Batch 230/1465, Loss: 1.1600\n",
      "Epoch 5/10, Batch 240/1465, Loss: 1.0077\n",
      "Epoch 5/10, Batch 250/1465, Loss: 0.8674\n",
      "Epoch 5/10, Batch 260/1465, Loss: 0.9185\n",
      "Epoch 5/10, Batch 270/1465, Loss: 1.1999\n",
      "Epoch 5/10, Batch 280/1465, Loss: 1.0912\n",
      "Epoch 5/10, Batch 290/1465, Loss: 1.1260\n",
      "Epoch 5/10, Batch 300/1465, Loss: 1.0445\n",
      "Epoch 5/10, Batch 310/1465, Loss: 1.1173\n",
      "Epoch 5/10, Batch 320/1465, Loss: 0.8559\n",
      "Epoch 5/10, Batch 330/1465, Loss: 0.7968\n",
      "Epoch 5/10, Batch 340/1465, Loss: 1.1519\n",
      "Epoch 5/10, Batch 350/1465, Loss: 0.9586\n",
      "Epoch 5/10, Batch 360/1465, Loss: 0.9843\n",
      "Epoch 5/10, Batch 370/1465, Loss: 1.0557\n",
      "Epoch 5/10, Batch 380/1465, Loss: 1.1519\n",
      "Epoch 5/10, Batch 390/1465, Loss: 0.7874\n",
      "Epoch 5/10, Batch 400/1465, Loss: 0.9866\n",
      "Epoch 5/10, Batch 410/1465, Loss: 1.2299\n",
      "Epoch 5/10, Batch 420/1465, Loss: 1.0037\n",
      "Epoch 5/10, Batch 430/1465, Loss: 1.0019\n",
      "Epoch 5/10, Batch 440/1465, Loss: 0.9019\n",
      "Epoch 5/10, Batch 450/1465, Loss: 1.1119\n",
      "Epoch 5/10, Batch 460/1465, Loss: 0.9148\n",
      "Epoch 5/10, Batch 470/1465, Loss: 0.8679\n",
      "Epoch 5/10, Batch 480/1465, Loss: 1.0451\n",
      "Epoch 5/10, Batch 490/1465, Loss: 1.1012\n",
      "Epoch 5/10, Batch 500/1465, Loss: 1.0656\n",
      "Epoch 5/10, Batch 510/1465, Loss: 0.9171\n",
      "Epoch 5/10, Batch 520/1465, Loss: 0.9514\n",
      "Epoch 5/10, Batch 530/1465, Loss: 0.9727\n",
      "Epoch 5/10, Batch 540/1465, Loss: 1.1016\n",
      "Epoch 5/10, Batch 550/1465, Loss: 1.0367\n",
      "Epoch 5/10, Batch 560/1465, Loss: 1.0742\n",
      "Epoch 5/10, Batch 570/1465, Loss: 1.1451\n",
      "Epoch 5/10, Batch 580/1465, Loss: 0.9968\n",
      "Epoch 5/10, Batch 590/1465, Loss: 1.1108\n",
      "Epoch 5/10, Batch 600/1465, Loss: 1.2296\n",
      "Epoch 5/10, Batch 610/1465, Loss: 0.8531\n",
      "Epoch 5/10, Batch 620/1465, Loss: 0.9660\n",
      "Epoch 5/10, Batch 630/1465, Loss: 0.9538\n",
      "Epoch 5/10, Batch 640/1465, Loss: 0.9183\n",
      "Epoch 5/10, Batch 650/1465, Loss: 0.9795\n",
      "Epoch 5/10, Batch 660/1465, Loss: 0.9445\n",
      "Epoch 5/10, Batch 670/1465, Loss: 0.8339\n",
      "Epoch 5/10, Batch 680/1465, Loss: 0.9974\n",
      "Epoch 5/10, Batch 690/1465, Loss: 1.1067\n",
      "Epoch 5/10, Batch 700/1465, Loss: 0.8542\n",
      "Epoch 5/10, Batch 710/1465, Loss: 1.1264\n",
      "Epoch 5/10, Batch 720/1465, Loss: 1.1840\n",
      "Epoch 5/10, Batch 730/1465, Loss: 0.9866\n",
      "Epoch 5/10, Batch 740/1465, Loss: 0.9496\n",
      "Epoch 5/10, Batch 750/1465, Loss: 1.1227\n",
      "Epoch 5/10, Batch 760/1465, Loss: 0.9592\n",
      "Epoch 5/10, Batch 770/1465, Loss: 1.1081\n",
      "Epoch 5/10, Batch 780/1465, Loss: 0.9802\n",
      "Epoch 5/10, Batch 790/1465, Loss: 1.3246\n",
      "Epoch 5/10, Batch 800/1465, Loss: 0.9520\n",
      "Epoch 5/10, Batch 810/1465, Loss: 1.1652\n",
      "Epoch 5/10, Batch 820/1465, Loss: 0.9986\n",
      "Epoch 5/10, Batch 830/1465, Loss: 0.7933\n",
      "Epoch 5/10, Batch 840/1465, Loss: 1.1353\n",
      "Epoch 5/10, Batch 850/1465, Loss: 0.9879\n",
      "Epoch 5/10, Batch 860/1465, Loss: 1.0358\n",
      "Epoch 5/10, Batch 870/1465, Loss: 0.9562\n",
      "Epoch 5/10, Batch 880/1465, Loss: 0.9168\n",
      "Epoch 5/10, Batch 890/1465, Loss: 0.9864\n",
      "Epoch 5/10, Batch 900/1465, Loss: 1.0449\n",
      "Epoch 5/10, Batch 910/1465, Loss: 1.1513\n",
      "Epoch 5/10, Batch 920/1465, Loss: 1.0298\n",
      "Epoch 5/10, Batch 930/1465, Loss: 1.1589\n",
      "Epoch 5/10, Batch 940/1465, Loss: 0.8563\n",
      "Epoch 5/10, Batch 950/1465, Loss: 1.2236\n",
      "Epoch 5/10, Batch 960/1465, Loss: 0.8564\n",
      "Epoch 5/10, Batch 970/1465, Loss: 1.0495\n",
      "Epoch 5/10, Batch 980/1465, Loss: 1.0088\n",
      "Epoch 5/10, Batch 990/1465, Loss: 1.0853\n",
      "Epoch 5/10, Batch 1000/1465, Loss: 1.1263\n",
      "Epoch 5/10, Batch 1010/1465, Loss: 1.1441\n",
      "Epoch 5/10, Batch 1020/1465, Loss: 1.2542\n",
      "Epoch 5/10, Batch 1030/1465, Loss: 0.9261\n",
      "Epoch 5/10, Batch 1040/1465, Loss: 1.1787\n",
      "Epoch 5/10, Batch 1050/1465, Loss: 0.7450\n",
      "Epoch 5/10, Batch 1060/1465, Loss: 1.1399\n",
      "Epoch 5/10, Batch 1070/1465, Loss: 0.8944\n",
      "Epoch 5/10, Batch 1080/1465, Loss: 1.4281\n",
      "Epoch 5/10, Batch 1090/1465, Loss: 0.8738\n",
      "Epoch 5/10, Batch 1100/1465, Loss: 0.8545\n",
      "Epoch 5/10, Batch 1110/1465, Loss: 0.9498\n",
      "Epoch 5/10, Batch 1120/1465, Loss: 0.8093\n",
      "Epoch 5/10, Batch 1130/1465, Loss: 1.0378\n",
      "Epoch 5/10, Batch 1140/1465, Loss: 0.9070\n",
      "Epoch 5/10, Batch 1150/1465, Loss: 1.0871\n",
      "Epoch 5/10, Batch 1160/1465, Loss: 1.1248\n",
      "Epoch 5/10, Batch 1170/1465, Loss: 1.0975\n",
      "Epoch 5/10, Batch 1180/1465, Loss: 1.0588\n",
      "Epoch 5/10, Batch 1190/1465, Loss: 1.0097\n",
      "Epoch 5/10, Batch 1200/1465, Loss: 1.1382\n",
      "Epoch 5/10, Batch 1210/1465, Loss: 1.1742\n",
      "Epoch 5/10, Batch 1220/1465, Loss: 1.1164\n",
      "Epoch 5/10, Batch 1230/1465, Loss: 0.9312\n",
      "Epoch 5/10, Batch 1240/1465, Loss: 0.9269\n",
      "Epoch 5/10, Batch 1250/1465, Loss: 1.0022\n",
      "Epoch 5/10, Batch 1260/1465, Loss: 0.7721\n",
      "Epoch 5/10, Batch 1270/1465, Loss: 1.0850\n",
      "Epoch 5/10, Batch 1280/1465, Loss: 0.9141\n",
      "Epoch 5/10, Batch 1290/1465, Loss: 1.0019\n",
      "Epoch 5/10, Batch 1300/1465, Loss: 0.8871\n",
      "Epoch 5/10, Batch 1310/1465, Loss: 0.8282\n",
      "Epoch 5/10, Batch 1320/1465, Loss: 1.0487\n",
      "Epoch 5/10, Batch 1330/1465, Loss: 1.1125\n",
      "Epoch 5/10, Batch 1340/1465, Loss: 1.0404\n",
      "Epoch 5/10, Batch 1350/1465, Loss: 1.1219\n",
      "Epoch 5/10, Batch 1360/1465, Loss: 0.9315\n",
      "Epoch 5/10, Batch 1370/1465, Loss: 0.9455\n",
      "Epoch 5/10, Batch 1380/1465, Loss: 1.1373\n",
      "Epoch 5/10, Batch 1390/1465, Loss: 0.6178\n",
      "Epoch 5/10, Batch 1400/1465, Loss: 0.9683\n",
      "Epoch 5/10, Batch 1410/1465, Loss: 0.9005\n",
      "Epoch 5/10, Batch 1420/1465, Loss: 1.1325\n",
      "Epoch 5/10, Batch 1430/1465, Loss: 1.0070\n",
      "Epoch 5/10, Batch 1440/1465, Loss: 1.0318\n",
      "Epoch 5/10, Batch 1450/1465, Loss: 0.8819\n",
      "Epoch 5/10, Batch 1460/1465, Loss: 0.9269\n",
      "Epoch [5/10], Average Loss: 1.0188\n",
      "Epoch 6/10, Batch 10/1465, Loss: 0.8734\n",
      "Epoch 6/10, Batch 20/1465, Loss: 1.0074\n",
      "Epoch 6/10, Batch 30/1465, Loss: 0.9831\n",
      "Epoch 6/10, Batch 40/1465, Loss: 0.9828\n",
      "Epoch 6/10, Batch 50/1465, Loss: 1.1948\n",
      "Epoch 6/10, Batch 60/1465, Loss: 0.9752\n",
      "Epoch 6/10, Batch 70/1465, Loss: 0.8897\n",
      "Epoch 6/10, Batch 80/1465, Loss: 1.0003\n",
      "Epoch 6/10, Batch 90/1465, Loss: 0.9721\n",
      "Epoch 6/10, Batch 100/1465, Loss: 0.9369\n",
      "Epoch 6/10, Batch 110/1465, Loss: 0.9550\n",
      "Epoch 6/10, Batch 120/1465, Loss: 1.0041\n",
      "Epoch 6/10, Batch 130/1465, Loss: 1.0045\n",
      "Epoch 6/10, Batch 140/1465, Loss: 0.9292\n",
      "Epoch 6/10, Batch 150/1465, Loss: 0.8523\n",
      "Epoch 6/10, Batch 160/1465, Loss: 1.0769\n",
      "Epoch 6/10, Batch 170/1465, Loss: 1.0588\n",
      "Epoch 6/10, Batch 180/1465, Loss: 0.8328\n",
      "Epoch 6/10, Batch 190/1465, Loss: 1.1289\n",
      "Epoch 6/10, Batch 200/1465, Loss: 1.0705\n",
      "Epoch 6/10, Batch 210/1465, Loss: 1.0217\n",
      "Epoch 6/10, Batch 220/1465, Loss: 0.8579\n",
      "Epoch 6/10, Batch 230/1465, Loss: 1.0301\n",
      "Epoch 6/10, Batch 240/1465, Loss: 1.2344\n",
      "Epoch 6/10, Batch 250/1465, Loss: 1.0360\n",
      "Epoch 6/10, Batch 260/1465, Loss: 1.0007\n",
      "Epoch 6/10, Batch 270/1465, Loss: 1.1726\n",
      "Epoch 6/10, Batch 280/1465, Loss: 1.1070\n",
      "Epoch 6/10, Batch 290/1465, Loss: 1.0534\n",
      "Epoch 6/10, Batch 300/1465, Loss: 1.0726\n",
      "Epoch 6/10, Batch 310/1465, Loss: 0.9405\n",
      "Epoch 6/10, Batch 320/1465, Loss: 0.8414\n",
      "Epoch 6/10, Batch 330/1465, Loss: 0.9050\n",
      "Epoch 6/10, Batch 340/1465, Loss: 0.9323\n",
      "Epoch 6/10, Batch 350/1465, Loss: 0.9836\n",
      "Epoch 6/10, Batch 360/1465, Loss: 0.9550\n",
      "Epoch 6/10, Batch 370/1465, Loss: 0.9943\n",
      "Epoch 6/10, Batch 380/1465, Loss: 0.9801\n",
      "Epoch 6/10, Batch 390/1465, Loss: 1.1156\n",
      "Epoch 6/10, Batch 400/1465, Loss: 0.9844\n",
      "Epoch 6/10, Batch 410/1465, Loss: 0.9165\n",
      "Epoch 6/10, Batch 420/1465, Loss: 0.9211\n",
      "Epoch 6/10, Batch 430/1465, Loss: 1.0939\n",
      "Epoch 6/10, Batch 440/1465, Loss: 0.8747\n",
      "Epoch 6/10, Batch 450/1465, Loss: 0.8516\n",
      "Epoch 6/10, Batch 460/1465, Loss: 1.0066\n",
      "Epoch 6/10, Batch 470/1465, Loss: 0.8931\n",
      "Epoch 6/10, Batch 480/1465, Loss: 0.8600\n",
      "Epoch 6/10, Batch 490/1465, Loss: 1.0309\n",
      "Epoch 6/10, Batch 500/1465, Loss: 1.0647\n",
      "Epoch 6/10, Batch 510/1465, Loss: 0.9280\n",
      "Epoch 6/10, Batch 520/1465, Loss: 1.0401\n",
      "Epoch 6/10, Batch 530/1465, Loss: 1.0632\n",
      "Epoch 6/10, Batch 540/1465, Loss: 1.0040\n",
      "Epoch 6/10, Batch 550/1465, Loss: 1.1277\n",
      "Epoch 6/10, Batch 560/1465, Loss: 0.9290\n",
      "Epoch 6/10, Batch 570/1465, Loss: 1.0349\n",
      "Epoch 6/10, Batch 580/1465, Loss: 0.9724\n",
      "Epoch 6/10, Batch 590/1465, Loss: 0.9647\n",
      "Epoch 6/10, Batch 600/1465, Loss: 0.9109\n",
      "Epoch 6/10, Batch 610/1465, Loss: 1.2718\n",
      "Epoch 6/10, Batch 620/1465, Loss: 0.7462\n",
      "Epoch 6/10, Batch 630/1465, Loss: 0.9424\n",
      "Epoch 6/10, Batch 640/1465, Loss: 0.9122\n",
      "Epoch 6/10, Batch 650/1465, Loss: 0.8924\n",
      "Epoch 6/10, Batch 660/1465, Loss: 0.9263\n",
      "Epoch 6/10, Batch 670/1465, Loss: 0.8789\n",
      "Epoch 6/10, Batch 680/1465, Loss: 1.1142\n",
      "Epoch 6/10, Batch 690/1465, Loss: 1.0041\n",
      "Epoch 6/10, Batch 700/1465, Loss: 0.9684\n",
      "Epoch 6/10, Batch 710/1465, Loss: 1.0035\n",
      "Epoch 6/10, Batch 720/1465, Loss: 0.9661\n",
      "Epoch 6/10, Batch 730/1465, Loss: 0.9067\n",
      "Epoch 6/10, Batch 740/1465, Loss: 0.7469\n",
      "Epoch 6/10, Batch 750/1465, Loss: 0.6684\n",
      "Epoch 6/10, Batch 760/1465, Loss: 0.8189\n",
      "Epoch 6/10, Batch 770/1465, Loss: 0.8283\n",
      "Epoch 6/10, Batch 780/1465, Loss: 0.9896\n",
      "Epoch 6/10, Batch 790/1465, Loss: 0.8831\n",
      "Epoch 6/10, Batch 800/1465, Loss: 1.0054\n",
      "Epoch 6/10, Batch 810/1465, Loss: 0.9493\n",
      "Epoch 6/10, Batch 820/1465, Loss: 0.9203\n",
      "Epoch 6/10, Batch 830/1465, Loss: 0.9839\n",
      "Epoch 6/10, Batch 840/1465, Loss: 0.9244\n",
      "Epoch 6/10, Batch 850/1465, Loss: 1.0249\n",
      "Epoch 6/10, Batch 860/1465, Loss: 1.1852\n",
      "Epoch 6/10, Batch 870/1465, Loss: 1.0292\n",
      "Epoch 6/10, Batch 880/1465, Loss: 1.1176\n",
      "Epoch 6/10, Batch 890/1465, Loss: 1.2031\n",
      "Epoch 6/10, Batch 900/1465, Loss: 0.9926\n",
      "Epoch 6/10, Batch 910/1465, Loss: 0.9502\n",
      "Epoch 6/10, Batch 920/1465, Loss: 0.9064\n",
      "Epoch 6/10, Batch 930/1465, Loss: 1.0029\n",
      "Epoch 6/10, Batch 940/1465, Loss: 1.1244\n",
      "Epoch 6/10, Batch 950/1465, Loss: 0.9711\n",
      "Epoch 6/10, Batch 960/1465, Loss: 0.9330\n",
      "Epoch 6/10, Batch 970/1465, Loss: 0.9523\n",
      "Epoch 6/10, Batch 980/1465, Loss: 0.9093\n",
      "Epoch 6/10, Batch 990/1465, Loss: 0.9351\n",
      "Epoch 6/10, Batch 1000/1465, Loss: 0.6949\n",
      "Epoch 6/10, Batch 1010/1465, Loss: 1.3056\n",
      "Epoch 6/10, Batch 1020/1465, Loss: 0.8531\n",
      "Epoch 6/10, Batch 1030/1465, Loss: 0.7757\n",
      "Epoch 6/10, Batch 1040/1465, Loss: 1.0319\n",
      "Epoch 6/10, Batch 1050/1465, Loss: 0.9044\n",
      "Epoch 6/10, Batch 1060/1465, Loss: 0.8808\n",
      "Epoch 6/10, Batch 1070/1465, Loss: 0.9276\n",
      "Epoch 6/10, Batch 1080/1465, Loss: 0.9951\n",
      "Epoch 6/10, Batch 1090/1465, Loss: 0.7922\n",
      "Epoch 6/10, Batch 1100/1465, Loss: 0.9320\n",
      "Epoch 6/10, Batch 1110/1465, Loss: 0.9888\n",
      "Epoch 6/10, Batch 1120/1465, Loss: 0.6955\n",
      "Epoch 6/10, Batch 1130/1465, Loss: 1.3009\n",
      "Epoch 6/10, Batch 1140/1465, Loss: 0.9783\n",
      "Epoch 6/10, Batch 1150/1465, Loss: 1.2491\n",
      "Epoch 6/10, Batch 1160/1465, Loss: 0.8644\n",
      "Epoch 6/10, Batch 1170/1465, Loss: 1.0439\n",
      "Epoch 6/10, Batch 1180/1465, Loss: 0.9996\n",
      "Epoch 6/10, Batch 1190/1465, Loss: 1.0096\n",
      "Epoch 6/10, Batch 1200/1465, Loss: 1.0419\n",
      "Epoch 6/10, Batch 1210/1465, Loss: 0.9806\n",
      "Epoch 6/10, Batch 1220/1465, Loss: 0.7291\n",
      "Epoch 6/10, Batch 1230/1465, Loss: 1.1472\n",
      "Epoch 6/10, Batch 1240/1465, Loss: 1.0541\n",
      "Epoch 6/10, Batch 1250/1465, Loss: 1.0515\n",
      "Epoch 6/10, Batch 1260/1465, Loss: 0.9947\n",
      "Epoch 6/10, Batch 1270/1465, Loss: 1.0872\n",
      "Epoch 6/10, Batch 1280/1465, Loss: 1.0257\n",
      "Epoch 6/10, Batch 1290/1465, Loss: 1.0133\n",
      "Epoch 6/10, Batch 1300/1465, Loss: 0.9373\n",
      "Epoch 6/10, Batch 1310/1465, Loss: 0.9621\n",
      "Epoch 6/10, Batch 1320/1465, Loss: 0.9565\n",
      "Epoch 6/10, Batch 1330/1465, Loss: 1.0338\n",
      "Epoch 6/10, Batch 1340/1465, Loss: 0.8498\n",
      "Epoch 6/10, Batch 1350/1465, Loss: 1.0227\n",
      "Epoch 6/10, Batch 1360/1465, Loss: 0.9953\n",
      "Epoch 6/10, Batch 1370/1465, Loss: 0.9880\n",
      "Epoch 6/10, Batch 1380/1465, Loss: 1.1004\n",
      "Epoch 6/10, Batch 1390/1465, Loss: 0.8711\n",
      "Epoch 6/10, Batch 1400/1465, Loss: 0.8467\n",
      "Epoch 6/10, Batch 1410/1465, Loss: 0.8408\n",
      "Epoch 6/10, Batch 1420/1465, Loss: 1.0022\n",
      "Epoch 6/10, Batch 1430/1465, Loss: 0.8436\n",
      "Epoch 6/10, Batch 1440/1465, Loss: 0.9439\n",
      "Epoch 6/10, Batch 1450/1465, Loss: 0.8992\n",
      "Epoch 6/10, Batch 1460/1465, Loss: 0.9884\n",
      "Epoch [6/10], Average Loss: 0.9965\n",
      "Epoch 7/10, Batch 10/1465, Loss: 1.0360\n",
      "Epoch 7/10, Batch 20/1465, Loss: 1.0823\n",
      "Epoch 7/10, Batch 30/1465, Loss: 0.9851\n",
      "Epoch 7/10, Batch 40/1465, Loss: 1.0576\n",
      "Epoch 7/10, Batch 50/1465, Loss: 0.9657\n",
      "Epoch 7/10, Batch 60/1465, Loss: 0.8866\n",
      "Epoch 7/10, Batch 70/1465, Loss: 1.1058\n",
      "Epoch 7/10, Batch 80/1465, Loss: 0.9425\n",
      "Epoch 7/10, Batch 90/1465, Loss: 0.9200\n",
      "Epoch 7/10, Batch 100/1465, Loss: 0.9272\n",
      "Epoch 7/10, Batch 110/1465, Loss: 0.9593\n",
      "Epoch 7/10, Batch 120/1465, Loss: 1.2719\n",
      "Epoch 7/10, Batch 130/1465, Loss: 1.2875\n",
      "Epoch 7/10, Batch 140/1465, Loss: 0.9501\n",
      "Epoch 7/10, Batch 150/1465, Loss: 0.9978\n",
      "Epoch 7/10, Batch 160/1465, Loss: 0.8645\n",
      "Epoch 7/10, Batch 170/1465, Loss: 0.7321\n",
      "Epoch 7/10, Batch 180/1465, Loss: 0.8877\n",
      "Epoch 7/10, Batch 190/1465, Loss: 0.9882\n",
      "Epoch 7/10, Batch 200/1465, Loss: 0.9568\n",
      "Epoch 7/10, Batch 210/1465, Loss: 0.8136\n",
      "Epoch 7/10, Batch 220/1465, Loss: 0.9344\n",
      "Epoch 7/10, Batch 230/1465, Loss: 0.9625\n",
      "Epoch 7/10, Batch 240/1465, Loss: 0.9650\n",
      "Epoch 7/10, Batch 250/1465, Loss: 0.9511\n",
      "Epoch 7/10, Batch 260/1465, Loss: 0.8824\n",
      "Epoch 7/10, Batch 270/1465, Loss: 0.9234\n",
      "Epoch 7/10, Batch 280/1465, Loss: 0.8788\n",
      "Epoch 7/10, Batch 290/1465, Loss: 0.8291\n",
      "Epoch 7/10, Batch 300/1465, Loss: 0.8958\n",
      "Epoch 7/10, Batch 310/1465, Loss: 1.0147\n",
      "Epoch 7/10, Batch 320/1465, Loss: 1.0288\n",
      "Epoch 7/10, Batch 330/1465, Loss: 0.9639\n",
      "Epoch 7/10, Batch 340/1465, Loss: 0.8392\n",
      "Epoch 7/10, Batch 350/1465, Loss: 1.1172\n",
      "Epoch 7/10, Batch 360/1465, Loss: 0.9558\n",
      "Epoch 7/10, Batch 370/1465, Loss: 0.7742\n",
      "Epoch 7/10, Batch 380/1465, Loss: 0.9374\n",
      "Epoch 7/10, Batch 390/1465, Loss: 0.9782\n",
      "Epoch 7/10, Batch 400/1465, Loss: 0.9924\n",
      "Epoch 7/10, Batch 410/1465, Loss: 1.0160\n",
      "Epoch 7/10, Batch 420/1465, Loss: 0.8483\n",
      "Epoch 7/10, Batch 430/1465, Loss: 0.8397\n",
      "Epoch 7/10, Batch 440/1465, Loss: 0.8173\n",
      "Epoch 7/10, Batch 450/1465, Loss: 1.0471\n",
      "Epoch 7/10, Batch 460/1465, Loss: 0.9763\n",
      "Epoch 7/10, Batch 470/1465, Loss: 1.1913\n",
      "Epoch 7/10, Batch 480/1465, Loss: 0.9778\n",
      "Epoch 7/10, Batch 490/1465, Loss: 0.8897\n",
      "Epoch 7/10, Batch 500/1465, Loss: 1.0256\n",
      "Epoch 7/10, Batch 510/1465, Loss: 1.0480\n",
      "Epoch 7/10, Batch 520/1465, Loss: 1.2486\n",
      "Epoch 7/10, Batch 530/1465, Loss: 0.8793\n",
      "Epoch 7/10, Batch 540/1465, Loss: 1.0732\n",
      "Epoch 7/10, Batch 550/1465, Loss: 0.9060\n",
      "Epoch 7/10, Batch 560/1465, Loss: 1.1535\n",
      "Epoch 7/10, Batch 570/1465, Loss: 0.8565\n",
      "Epoch 7/10, Batch 580/1465, Loss: 0.6498\n",
      "Epoch 7/10, Batch 590/1465, Loss: 0.8472\n",
      "Epoch 7/10, Batch 600/1465, Loss: 1.0142\n",
      "Epoch 7/10, Batch 610/1465, Loss: 0.8783\n",
      "Epoch 7/10, Batch 620/1465, Loss: 1.0578\n",
      "Epoch 7/10, Batch 630/1465, Loss: 1.1107\n",
      "Epoch 7/10, Batch 640/1465, Loss: 0.8871\n",
      "Epoch 7/10, Batch 650/1465, Loss: 0.9876\n",
      "Epoch 7/10, Batch 660/1465, Loss: 0.7401\n",
      "Epoch 7/10, Batch 670/1465, Loss: 0.8356\n",
      "Epoch 7/10, Batch 680/1465, Loss: 0.9079\n",
      "Epoch 7/10, Batch 690/1465, Loss: 0.8569\n",
      "Epoch 7/10, Batch 700/1465, Loss: 0.9935\n",
      "Epoch 7/10, Batch 710/1465, Loss: 1.0165\n",
      "Epoch 7/10, Batch 720/1465, Loss: 1.0552\n",
      "Epoch 7/10, Batch 730/1465, Loss: 1.2971\n",
      "Epoch 7/10, Batch 740/1465, Loss: 1.2447\n",
      "Epoch 7/10, Batch 750/1465, Loss: 0.9938\n",
      "Epoch 7/10, Batch 760/1465, Loss: 1.1509\n",
      "Epoch 7/10, Batch 770/1465, Loss: 0.9575\n",
      "Epoch 7/10, Batch 780/1465, Loss: 0.7704\n",
      "Epoch 7/10, Batch 790/1465, Loss: 0.8875\n",
      "Epoch 7/10, Batch 800/1465, Loss: 0.8685\n",
      "Epoch 7/10, Batch 810/1465, Loss: 0.9551\n",
      "Epoch 7/10, Batch 820/1465, Loss: 0.8951\n",
      "Epoch 7/10, Batch 830/1465, Loss: 1.0274\n",
      "Epoch 7/10, Batch 840/1465, Loss: 1.0831\n",
      "Epoch 7/10, Batch 850/1465, Loss: 1.0824\n",
      "Epoch 7/10, Batch 860/1465, Loss: 0.9114\n",
      "Epoch 7/10, Batch 870/1465, Loss: 1.0531\n",
      "Epoch 7/10, Batch 880/1465, Loss: 1.0199\n",
      "Epoch 7/10, Batch 890/1465, Loss: 0.9678\n",
      "Epoch 7/10, Batch 900/1465, Loss: 0.7992\n",
      "Epoch 7/10, Batch 910/1465, Loss: 0.9740\n",
      "Epoch 7/10, Batch 920/1465, Loss: 0.8533\n",
      "Epoch 7/10, Batch 930/1465, Loss: 0.8582\n",
      "Epoch 7/10, Batch 940/1465, Loss: 0.9289\n",
      "Epoch 7/10, Batch 950/1465, Loss: 0.8709\n",
      "Epoch 7/10, Batch 960/1465, Loss: 1.1676\n",
      "Epoch 7/10, Batch 970/1465, Loss: 1.0469\n",
      "Epoch 7/10, Batch 980/1465, Loss: 1.0337\n",
      "Epoch 7/10, Batch 990/1465, Loss: 0.9878\n",
      "Epoch 7/10, Batch 1000/1465, Loss: 0.8291\n",
      "Epoch 7/10, Batch 1010/1465, Loss: 1.1224\n",
      "Epoch 7/10, Batch 1020/1465, Loss: 0.9214\n",
      "Epoch 7/10, Batch 1030/1465, Loss: 0.8295\n",
      "Epoch 7/10, Batch 1040/1465, Loss: 0.9768\n",
      "Epoch 7/10, Batch 1050/1465, Loss: 1.1430\n",
      "Epoch 7/10, Batch 1060/1465, Loss: 1.0389\n",
      "Epoch 7/10, Batch 1070/1465, Loss: 0.9654\n",
      "Epoch 7/10, Batch 1080/1465, Loss: 0.6626\n",
      "Epoch 7/10, Batch 1090/1465, Loss: 0.8984\n",
      "Epoch 7/10, Batch 1100/1465, Loss: 1.0020\n",
      "Epoch 7/10, Batch 1110/1465, Loss: 0.9202\n",
      "Epoch 7/10, Batch 1120/1465, Loss: 1.1012\n",
      "Epoch 7/10, Batch 1130/1465, Loss: 1.0216\n",
      "Epoch 7/10, Batch 1140/1465, Loss: 0.8218\n",
      "Epoch 7/10, Batch 1150/1465, Loss: 1.0596\n",
      "Epoch 7/10, Batch 1160/1465, Loss: 0.9167\n",
      "Epoch 7/10, Batch 1170/1465, Loss: 0.9165\n",
      "Epoch 7/10, Batch 1180/1465, Loss: 1.1007\n",
      "Epoch 7/10, Batch 1190/1465, Loss: 0.9504\n",
      "Epoch 7/10, Batch 1200/1465, Loss: 0.9927\n",
      "Epoch 7/10, Batch 1210/1465, Loss: 0.9748\n",
      "Epoch 7/10, Batch 1220/1465, Loss: 0.9942\n",
      "Epoch 7/10, Batch 1230/1465, Loss: 0.9366\n",
      "Epoch 7/10, Batch 1240/1465, Loss: 1.1250\n",
      "Epoch 7/10, Batch 1250/1465, Loss: 0.9127\n",
      "Epoch 7/10, Batch 1260/1465, Loss: 0.8832\n",
      "Epoch 7/10, Batch 1270/1465, Loss: 1.0247\n",
      "Epoch 7/10, Batch 1280/1465, Loss: 0.8598\n",
      "Epoch 7/10, Batch 1290/1465, Loss: 0.7046\n",
      "Epoch 7/10, Batch 1300/1465, Loss: 0.8673\n",
      "Epoch 7/10, Batch 1310/1465, Loss: 0.9889\n",
      "Epoch 7/10, Batch 1320/1465, Loss: 1.2289\n",
      "Epoch 7/10, Batch 1330/1465, Loss: 1.0684\n",
      "Epoch 7/10, Batch 1340/1465, Loss: 0.9001\n",
      "Epoch 7/10, Batch 1350/1465, Loss: 1.1632\n",
      "Epoch 7/10, Batch 1360/1465, Loss: 0.7566\n",
      "Epoch 7/10, Batch 1370/1465, Loss: 0.9758\n",
      "Epoch 7/10, Batch 1380/1465, Loss: 1.0024\n",
      "Epoch 7/10, Batch 1390/1465, Loss: 1.1498\n",
      "Epoch 7/10, Batch 1400/1465, Loss: 1.0272\n",
      "Epoch 7/10, Batch 1410/1465, Loss: 0.9922\n",
      "Epoch 7/10, Batch 1420/1465, Loss: 0.9741\n",
      "Epoch 7/10, Batch 1430/1465, Loss: 0.8445\n",
      "Epoch 7/10, Batch 1440/1465, Loss: 1.0330\n",
      "Epoch 7/10, Batch 1450/1465, Loss: 0.8601\n",
      "Epoch 7/10, Batch 1460/1465, Loss: 0.9899\n",
      "Epoch [7/10], Average Loss: 0.9852\n",
      "Epoch 8/10, Batch 10/1465, Loss: 0.9723\n",
      "Epoch 8/10, Batch 20/1465, Loss: 1.0490\n",
      "Epoch 8/10, Batch 30/1465, Loss: 0.8325\n",
      "Epoch 8/10, Batch 40/1465, Loss: 0.9713\n",
      "Epoch 8/10, Batch 50/1465, Loss: 1.0458\n",
      "Epoch 8/10, Batch 60/1465, Loss: 0.9656\n",
      "Epoch 8/10, Batch 70/1465, Loss: 0.9078\n",
      "Epoch 8/10, Batch 80/1465, Loss: 0.8000\n",
      "Epoch 8/10, Batch 90/1465, Loss: 0.9471\n",
      "Epoch 8/10, Batch 100/1465, Loss: 0.9414\n",
      "Epoch 8/10, Batch 110/1465, Loss: 0.8005\n",
      "Epoch 8/10, Batch 120/1465, Loss: 0.7261\n",
      "Epoch 8/10, Batch 130/1465, Loss: 0.9057\n",
      "Epoch 8/10, Batch 140/1465, Loss: 0.9873\n",
      "Epoch 8/10, Batch 150/1465, Loss: 1.1047\n",
      "Epoch 8/10, Batch 160/1465, Loss: 1.0626\n",
      "Epoch 8/10, Batch 170/1465, Loss: 1.0746\n",
      "Epoch 8/10, Batch 180/1465, Loss: 0.9731\n",
      "Epoch 8/10, Batch 190/1465, Loss: 0.7889\n",
      "Epoch 8/10, Batch 200/1465, Loss: 0.9139\n",
      "Epoch 8/10, Batch 210/1465, Loss: 1.3283\n",
      "Epoch 8/10, Batch 220/1465, Loss: 1.0433\n",
      "Epoch 8/10, Batch 230/1465, Loss: 0.8636\n",
      "Epoch 8/10, Batch 240/1465, Loss: 1.1046\n",
      "Epoch 8/10, Batch 250/1465, Loss: 0.9716\n",
      "Epoch 8/10, Batch 260/1465, Loss: 1.1056\n",
      "Epoch 8/10, Batch 270/1465, Loss: 0.9617\n",
      "Epoch 8/10, Batch 280/1465, Loss: 0.9219\n",
      "Epoch 8/10, Batch 290/1465, Loss: 1.0325\n",
      "Epoch 8/10, Batch 300/1465, Loss: 0.9219\n",
      "Epoch 8/10, Batch 310/1465, Loss: 1.1980\n",
      "Epoch 8/10, Batch 320/1465, Loss: 0.8328\n",
      "Epoch 8/10, Batch 330/1465, Loss: 1.1038\n",
      "Epoch 8/10, Batch 340/1465, Loss: 1.0955\n",
      "Epoch 8/10, Batch 350/1465, Loss: 0.9537\n",
      "Epoch 8/10, Batch 360/1465, Loss: 0.7596\n",
      "Epoch 8/10, Batch 370/1465, Loss: 0.8690\n",
      "Epoch 8/10, Batch 380/1465, Loss: 0.8761\n",
      "Epoch 8/10, Batch 390/1465, Loss: 1.0994\n",
      "Epoch 8/10, Batch 400/1465, Loss: 1.0644\n",
      "Epoch 8/10, Batch 410/1465, Loss: 0.9822\n",
      "Epoch 8/10, Batch 420/1465, Loss: 1.0658\n",
      "Epoch 8/10, Batch 430/1465, Loss: 1.0741\n",
      "Epoch 8/10, Batch 440/1465, Loss: 1.0039\n",
      "Epoch 8/10, Batch 450/1465, Loss: 1.0138\n",
      "Epoch 8/10, Batch 460/1465, Loss: 1.0514\n",
      "Epoch 8/10, Batch 470/1465, Loss: 0.8508\n",
      "Epoch 8/10, Batch 480/1465, Loss: 1.0780\n",
      "Epoch 8/10, Batch 490/1465, Loss: 0.9570\n",
      "Epoch 8/10, Batch 500/1465, Loss: 0.9272\n",
      "Epoch 8/10, Batch 510/1465, Loss: 1.0258\n",
      "Epoch 8/10, Batch 520/1465, Loss: 1.2067\n",
      "Epoch 8/10, Batch 530/1465, Loss: 1.0081\n",
      "Epoch 8/10, Batch 540/1465, Loss: 0.9518\n",
      "Epoch 8/10, Batch 550/1465, Loss: 1.0881\n",
      "Epoch 8/10, Batch 560/1465, Loss: 1.0484\n",
      "Epoch 8/10, Batch 570/1465, Loss: 0.8495\n",
      "Epoch 8/10, Batch 580/1465, Loss: 0.9362\n",
      "Epoch 8/10, Batch 590/1465, Loss: 0.9180\n",
      "Epoch 8/10, Batch 600/1465, Loss: 1.1136\n",
      "Epoch 8/10, Batch 610/1465, Loss: 0.9475\n",
      "Epoch 8/10, Batch 620/1465, Loss: 0.9587\n",
      "Epoch 8/10, Batch 630/1465, Loss: 1.1732\n",
      "Epoch 8/10, Batch 640/1465, Loss: 1.0675\n",
      "Epoch 8/10, Batch 650/1465, Loss: 1.1379\n",
      "Epoch 8/10, Batch 660/1465, Loss: 1.1069\n",
      "Epoch 8/10, Batch 670/1465, Loss: 0.8742\n",
      "Epoch 8/10, Batch 680/1465, Loss: 1.0834\n",
      "Epoch 8/10, Batch 690/1465, Loss: 0.8490\n",
      "Epoch 8/10, Batch 700/1465, Loss: 0.6876\n",
      "Epoch 8/10, Batch 710/1465, Loss: 0.8891\n",
      "Epoch 8/10, Batch 720/1465, Loss: 0.9715\n",
      "Epoch 8/10, Batch 730/1465, Loss: 1.1067\n",
      "Epoch 8/10, Batch 740/1465, Loss: 0.8750\n",
      "Epoch 8/10, Batch 750/1465, Loss: 1.1060\n",
      "Epoch 8/10, Batch 760/1465, Loss: 0.8820\n",
      "Epoch 8/10, Batch 770/1465, Loss: 0.9159\n",
      "Epoch 8/10, Batch 780/1465, Loss: 1.0120\n",
      "Epoch 8/10, Batch 790/1465, Loss: 0.7990\n",
      "Epoch 8/10, Batch 800/1465, Loss: 0.8612\n",
      "Epoch 8/10, Batch 810/1465, Loss: 0.9280\n",
      "Epoch 8/10, Batch 820/1465, Loss: 0.9458\n",
      "Epoch 8/10, Batch 830/1465, Loss: 0.7474\n",
      "Epoch 8/10, Batch 840/1465, Loss: 1.0405\n",
      "Epoch 8/10, Batch 850/1465, Loss: 0.8713\n",
      "Epoch 8/10, Batch 860/1465, Loss: 0.8493\n",
      "Epoch 8/10, Batch 870/1465, Loss: 0.9925\n",
      "Epoch 8/10, Batch 880/1465, Loss: 0.9592\n",
      "Epoch 8/10, Batch 890/1465, Loss: 0.9899\n",
      "Epoch 8/10, Batch 900/1465, Loss: 1.0255\n",
      "Epoch 8/10, Batch 910/1465, Loss: 1.2444\n",
      "Epoch 8/10, Batch 920/1465, Loss: 0.7897\n",
      "Epoch 8/10, Batch 930/1465, Loss: 0.8244\n",
      "Epoch 8/10, Batch 940/1465, Loss: 1.0412\n",
      "Epoch 8/10, Batch 950/1465, Loss: 0.9885\n",
      "Epoch 8/10, Batch 960/1465, Loss: 0.9802\n",
      "Epoch 8/10, Batch 970/1465, Loss: 1.0049\n",
      "Epoch 8/10, Batch 980/1465, Loss: 0.9129\n",
      "Epoch 8/10, Batch 990/1465, Loss: 0.8949\n",
      "Epoch 8/10, Batch 1000/1465, Loss: 1.0179\n",
      "Epoch 8/10, Batch 1010/1465, Loss: 0.9637\n",
      "Epoch 8/10, Batch 1020/1465, Loss: 0.8570\n",
      "Epoch 8/10, Batch 1030/1465, Loss: 1.0969\n",
      "Epoch 8/10, Batch 1040/1465, Loss: 0.8216\n",
      "Epoch 8/10, Batch 1050/1465, Loss: 0.9434\n",
      "Epoch 8/10, Batch 1060/1465, Loss: 1.1352\n",
      "Epoch 8/10, Batch 1070/1465, Loss: 0.9471\n",
      "Epoch 8/10, Batch 1080/1465, Loss: 0.9327\n",
      "Epoch 8/10, Batch 1090/1465, Loss: 0.9759\n",
      "Epoch 8/10, Batch 1100/1465, Loss: 0.7632\n",
      "Epoch 8/10, Batch 1110/1465, Loss: 1.1410\n",
      "Epoch 8/10, Batch 1120/1465, Loss: 1.1323\n",
      "Epoch 8/10, Batch 1130/1465, Loss: 0.8835\n",
      "Epoch 8/10, Batch 1140/1465, Loss: 0.9309\n",
      "Epoch 8/10, Batch 1150/1465, Loss: 1.1608\n",
      "Epoch 8/10, Batch 1160/1465, Loss: 0.8758\n",
      "Epoch 8/10, Batch 1170/1465, Loss: 0.9589\n",
      "Epoch 8/10, Batch 1180/1465, Loss: 0.9249\n",
      "Epoch 8/10, Batch 1190/1465, Loss: 1.0894\n",
      "Epoch 8/10, Batch 1200/1465, Loss: 0.9516\n",
      "Epoch 8/10, Batch 1210/1465, Loss: 0.9535\n",
      "Epoch 8/10, Batch 1220/1465, Loss: 0.8135\n",
      "Epoch 8/10, Batch 1230/1465, Loss: 1.0718\n",
      "Epoch 8/10, Batch 1240/1465, Loss: 0.9525\n",
      "Epoch 8/10, Batch 1250/1465, Loss: 1.0114\n",
      "Epoch 8/10, Batch 1260/1465, Loss: 1.0085\n",
      "Epoch 8/10, Batch 1270/1465, Loss: 1.0211\n",
      "Epoch 8/10, Batch 1280/1465, Loss: 1.2230\n",
      "Epoch 8/10, Batch 1290/1465, Loss: 1.0359\n",
      "Epoch 8/10, Batch 1300/1465, Loss: 1.0169\n",
      "Epoch 8/10, Batch 1310/1465, Loss: 0.9465\n",
      "Epoch 8/10, Batch 1320/1465, Loss: 0.8468\n",
      "Epoch 8/10, Batch 1330/1465, Loss: 0.9104\n",
      "Epoch 8/10, Batch 1340/1465, Loss: 1.1507\n",
      "Epoch 8/10, Batch 1350/1465, Loss: 0.7439\n",
      "Epoch 8/10, Batch 1360/1465, Loss: 0.7740\n",
      "Epoch 8/10, Batch 1370/1465, Loss: 0.8160\n",
      "Epoch 8/10, Batch 1380/1465, Loss: 0.9859\n",
      "Epoch 8/10, Batch 1390/1465, Loss: 0.7649\n",
      "Epoch 8/10, Batch 1400/1465, Loss: 1.0462\n",
      "Epoch 8/10, Batch 1410/1465, Loss: 1.0661\n",
      "Epoch 8/10, Batch 1420/1465, Loss: 1.0605\n",
      "Epoch 8/10, Batch 1430/1465, Loss: 0.9731\n",
      "Epoch 8/10, Batch 1440/1465, Loss: 1.0322\n",
      "Epoch 8/10, Batch 1450/1465, Loss: 1.0388\n",
      "Epoch 8/10, Batch 1460/1465, Loss: 0.8909\n",
      "Epoch [8/10], Average Loss: 0.9789\n",
      "Epoch 9/10, Batch 10/1465, Loss: 0.9295\n",
      "Epoch 9/10, Batch 20/1465, Loss: 0.9001\n",
      "Epoch 9/10, Batch 30/1465, Loss: 0.8633\n",
      "Epoch 9/10, Batch 40/1465, Loss: 1.1386\n",
      "Epoch 9/10, Batch 50/1465, Loss: 0.9550\n",
      "Epoch 9/10, Batch 60/1465, Loss: 1.0694\n",
      "Epoch 9/10, Batch 70/1465, Loss: 0.9347\n",
      "Epoch 9/10, Batch 80/1465, Loss: 1.1631\n",
      "Epoch 9/10, Batch 90/1465, Loss: 1.0949\n",
      "Epoch 9/10, Batch 100/1465, Loss: 0.8335\n",
      "Epoch 9/10, Batch 110/1465, Loss: 1.0292\n",
      "Epoch 9/10, Batch 120/1465, Loss: 0.9800\n",
      "Epoch 9/10, Batch 130/1465, Loss: 1.1664\n",
      "Epoch 9/10, Batch 140/1465, Loss: 0.9785\n",
      "Epoch 9/10, Batch 150/1465, Loss: 1.0500\n",
      "Epoch 9/10, Batch 160/1465, Loss: 0.8093\n",
      "Epoch 9/10, Batch 170/1465, Loss: 1.0829\n",
      "Epoch 9/10, Batch 180/1465, Loss: 1.1029\n",
      "Epoch 9/10, Batch 190/1465, Loss: 0.8016\n",
      "Epoch 9/10, Batch 200/1465, Loss: 0.8420\n",
      "Epoch 9/10, Batch 210/1465, Loss: 0.8957\n",
      "Epoch 9/10, Batch 220/1465, Loss: 0.9648\n",
      "Epoch 9/10, Batch 230/1465, Loss: 1.0120\n",
      "Epoch 9/10, Batch 240/1465, Loss: 0.9396\n",
      "Epoch 9/10, Batch 250/1465, Loss: 0.7997\n",
      "Epoch 9/10, Batch 260/1465, Loss: 0.7502\n",
      "Epoch 9/10, Batch 270/1465, Loss: 1.0128\n",
      "Epoch 9/10, Batch 280/1465, Loss: 1.0014\n",
      "Epoch 9/10, Batch 290/1465, Loss: 1.0036\n",
      "Epoch 9/10, Batch 300/1465, Loss: 0.8734\n",
      "Epoch 9/10, Batch 310/1465, Loss: 1.0365\n",
      "Epoch 9/10, Batch 320/1465, Loss: 0.9808\n",
      "Epoch 9/10, Batch 330/1465, Loss: 0.8180\n",
      "Epoch 9/10, Batch 340/1465, Loss: 0.9662\n",
      "Epoch 9/10, Batch 350/1465, Loss: 1.1079\n",
      "Epoch 9/10, Batch 360/1465, Loss: 0.9018\n",
      "Epoch 9/10, Batch 370/1465, Loss: 0.9213\n",
      "Epoch 9/10, Batch 380/1465, Loss: 0.8720\n",
      "Epoch 9/10, Batch 390/1465, Loss: 0.7062\n",
      "Epoch 9/10, Batch 400/1465, Loss: 1.0479\n",
      "Epoch 9/10, Batch 410/1465, Loss: 0.8518\n",
      "Epoch 9/10, Batch 420/1465, Loss: 0.7059\n",
      "Epoch 9/10, Batch 430/1465, Loss: 0.9612\n",
      "Epoch 9/10, Batch 440/1465, Loss: 0.9665\n",
      "Epoch 9/10, Batch 450/1465, Loss: 0.8254\n",
      "Epoch 9/10, Batch 460/1465, Loss: 0.9061\n",
      "Epoch 9/10, Batch 470/1465, Loss: 1.1867\n",
      "Epoch 9/10, Batch 480/1465, Loss: 1.0346\n",
      "Epoch 9/10, Batch 490/1465, Loss: 0.8810\n",
      "Epoch 9/10, Batch 500/1465, Loss: 1.0564\n",
      "Epoch 9/10, Batch 510/1465, Loss: 0.8865\n",
      "Epoch 9/10, Batch 520/1465, Loss: 1.0426\n",
      "Epoch 9/10, Batch 530/1465, Loss: 0.9182\n",
      "Epoch 9/10, Batch 540/1465, Loss: 1.0563\n",
      "Epoch 9/10, Batch 550/1465, Loss: 1.0183\n",
      "Epoch 9/10, Batch 560/1465, Loss: 0.8860\n",
      "Epoch 9/10, Batch 570/1465, Loss: 0.9887\n",
      "Epoch 9/10, Batch 580/1465, Loss: 1.0224\n",
      "Epoch 9/10, Batch 590/1465, Loss: 1.0274\n",
      "Epoch 9/10, Batch 600/1465, Loss: 0.8708\n",
      "Epoch 9/10, Batch 610/1465, Loss: 0.9238\n",
      "Epoch 9/10, Batch 620/1465, Loss: 0.7922\n",
      "Epoch 9/10, Batch 630/1465, Loss: 1.0840\n",
      "Epoch 9/10, Batch 640/1465, Loss: 0.9275\n",
      "Epoch 9/10, Batch 650/1465, Loss: 1.0275\n",
      "Epoch 9/10, Batch 660/1465, Loss: 1.1037\n",
      "Epoch 9/10, Batch 670/1465, Loss: 1.1725\n",
      "Epoch 9/10, Batch 680/1465, Loss: 0.8511\n",
      "Epoch 9/10, Batch 690/1465, Loss: 1.0843\n",
      "Epoch 9/10, Batch 700/1465, Loss: 1.0183\n",
      "Epoch 9/10, Batch 710/1465, Loss: 1.0388\n",
      "Epoch 9/10, Batch 720/1465, Loss: 1.0266\n",
      "Epoch 9/10, Batch 730/1465, Loss: 0.8965\n",
      "Epoch 9/10, Batch 740/1465, Loss: 0.8983\n",
      "Epoch 9/10, Batch 750/1465, Loss: 0.8769\n",
      "Epoch 9/10, Batch 760/1465, Loss: 1.0229\n",
      "Epoch 9/10, Batch 770/1465, Loss: 0.9771\n",
      "Epoch 9/10, Batch 780/1465, Loss: 0.9506\n",
      "Epoch 9/10, Batch 790/1465, Loss: 1.0449\n",
      "Epoch 9/10, Batch 800/1465, Loss: 0.9557\n",
      "Epoch 9/10, Batch 810/1465, Loss: 1.0314\n",
      "Epoch 9/10, Batch 820/1465, Loss: 0.9443\n",
      "Epoch 9/10, Batch 830/1465, Loss: 0.7953\n",
      "Epoch 9/10, Batch 840/1465, Loss: 0.9412\n",
      "Epoch 9/10, Batch 850/1465, Loss: 1.0996\n",
      "Epoch 9/10, Batch 860/1465, Loss: 0.8949\n",
      "Epoch 9/10, Batch 870/1465, Loss: 0.8899\n",
      "Epoch 9/10, Batch 880/1465, Loss: 1.1278\n",
      "Epoch 9/10, Batch 890/1465, Loss: 0.8737\n",
      "Epoch 9/10, Batch 900/1465, Loss: 1.0097\n",
      "Epoch 9/10, Batch 910/1465, Loss: 1.0754\n",
      "Epoch 9/10, Batch 920/1465, Loss: 1.3047\n",
      "Epoch 9/10, Batch 930/1465, Loss: 0.9580\n",
      "Epoch 9/10, Batch 940/1465, Loss: 0.9101\n",
      "Epoch 9/10, Batch 950/1465, Loss: 0.7294\n",
      "Epoch 9/10, Batch 960/1465, Loss: 0.8430\n",
      "Epoch 9/10, Batch 970/1465, Loss: 1.0096\n",
      "Epoch 9/10, Batch 980/1465, Loss: 0.9350\n",
      "Epoch 9/10, Batch 990/1465, Loss: 0.7566\n",
      "Epoch 9/10, Batch 1000/1465, Loss: 0.9637\n",
      "Epoch 9/10, Batch 1010/1465, Loss: 0.9840\n",
      "Epoch 9/10, Batch 1020/1465, Loss: 0.7686\n",
      "Epoch 9/10, Batch 1030/1465, Loss: 0.8572\n",
      "Epoch 9/10, Batch 1040/1465, Loss: 0.9177\n",
      "Epoch 9/10, Batch 1050/1465, Loss: 0.8151\n",
      "Epoch 9/10, Batch 1060/1465, Loss: 0.8291\n",
      "Epoch 9/10, Batch 1070/1465, Loss: 1.0962\n",
      "Epoch 9/10, Batch 1080/1465, Loss: 0.8299\n",
      "Epoch 9/10, Batch 1090/1465, Loss: 0.9735\n",
      "Epoch 9/10, Batch 1100/1465, Loss: 1.0132\n",
      "Epoch 9/10, Batch 1110/1465, Loss: 0.9960\n",
      "Epoch 9/10, Batch 1120/1465, Loss: 1.0775\n",
      "Epoch 9/10, Batch 1130/1465, Loss: 0.8246\n",
      "Epoch 9/10, Batch 1140/1465, Loss: 0.9676\n",
      "Epoch 9/10, Batch 1150/1465, Loss: 0.9541\n",
      "Epoch 9/10, Batch 1160/1465, Loss: 1.0188\n",
      "Epoch 9/10, Batch 1170/1465, Loss: 1.0440\n",
      "Epoch 9/10, Batch 1180/1465, Loss: 1.0216\n",
      "Epoch 9/10, Batch 1190/1465, Loss: 0.9281\n",
      "Epoch 9/10, Batch 1200/1465, Loss: 1.1397\n",
      "Epoch 9/10, Batch 1210/1465, Loss: 1.0325\n",
      "Epoch 9/10, Batch 1220/1465, Loss: 0.9901\n",
      "Epoch 9/10, Batch 1230/1465, Loss: 0.9797\n",
      "Epoch 9/10, Batch 1240/1465, Loss: 0.8165\n",
      "Epoch 9/10, Batch 1250/1465, Loss: 0.7483\n",
      "Epoch 9/10, Batch 1260/1465, Loss: 0.9834\n",
      "Epoch 9/10, Batch 1270/1465, Loss: 1.1213\n",
      "Epoch 9/10, Batch 1280/1465, Loss: 0.8095\n",
      "Epoch 9/10, Batch 1290/1465, Loss: 1.2149\n",
      "Epoch 9/10, Batch 1300/1465, Loss: 0.9240\n",
      "Epoch 9/10, Batch 1310/1465, Loss: 0.8634\n",
      "Epoch 9/10, Batch 1320/1465, Loss: 0.8746\n",
      "Epoch 9/10, Batch 1330/1465, Loss: 0.8411\n",
      "Epoch 9/10, Batch 1340/1465, Loss: 1.0215\n",
      "Epoch 9/10, Batch 1350/1465, Loss: 1.0588\n",
      "Epoch 9/10, Batch 1360/1465, Loss: 1.0463\n",
      "Epoch 9/10, Batch 1370/1465, Loss: 0.9768\n",
      "Epoch 9/10, Batch 1380/1465, Loss: 1.0601\n",
      "Epoch 9/10, Batch 1390/1465, Loss: 0.7616\n",
      "Epoch 9/10, Batch 1400/1465, Loss: 0.8507\n",
      "Epoch 9/10, Batch 1410/1465, Loss: 0.8007\n",
      "Epoch 9/10, Batch 1420/1465, Loss: 0.7969\n",
      "Epoch 9/10, Batch 1430/1465, Loss: 1.0710\n",
      "Epoch 9/10, Batch 1440/1465, Loss: 0.9941\n",
      "Epoch 9/10, Batch 1450/1465, Loss: 0.6632\n",
      "Epoch 9/10, Batch 1460/1465, Loss: 1.0634\n",
      "Epoch [9/10], Average Loss: 0.9710\n",
      "Epoch 10/10, Batch 10/1465, Loss: 0.8959\n",
      "Epoch 10/10, Batch 20/1465, Loss: 0.9914\n",
      "Epoch 10/10, Batch 30/1465, Loss: 1.0299\n",
      "Epoch 10/10, Batch 40/1465, Loss: 0.9310\n",
      "Epoch 10/10, Batch 50/1465, Loss: 0.8181\n",
      "Epoch 10/10, Batch 60/1465, Loss: 1.0864\n",
      "Epoch 10/10, Batch 70/1465, Loss: 0.8205\n",
      "Epoch 10/10, Batch 80/1465, Loss: 0.8112\n",
      "Epoch 10/10, Batch 90/1465, Loss: 0.8461\n",
      "Epoch 10/10, Batch 100/1465, Loss: 0.9620\n",
      "Epoch 10/10, Batch 110/1465, Loss: 1.2059\n",
      "Epoch 10/10, Batch 120/1465, Loss: 0.8371\n",
      "Epoch 10/10, Batch 130/1465, Loss: 0.8028\n",
      "Epoch 10/10, Batch 140/1465, Loss: 1.0324\n",
      "Epoch 10/10, Batch 150/1465, Loss: 0.9911\n",
      "Epoch 10/10, Batch 160/1465, Loss: 1.2754\n",
      "Epoch 10/10, Batch 170/1465, Loss: 0.8737\n",
      "Epoch 10/10, Batch 180/1465, Loss: 1.1976\n",
      "Epoch 10/10, Batch 190/1465, Loss: 0.7400\n",
      "Epoch 10/10, Batch 200/1465, Loss: 1.1416\n",
      "Epoch 10/10, Batch 210/1465, Loss: 0.8576\n",
      "Epoch 10/10, Batch 220/1465, Loss: 0.9056\n",
      "Epoch 10/10, Batch 230/1465, Loss: 0.8391\n",
      "Epoch 10/10, Batch 240/1465, Loss: 0.7969\n",
      "Epoch 10/10, Batch 250/1465, Loss: 0.9728\n",
      "Epoch 10/10, Batch 260/1465, Loss: 0.9342\n",
      "Epoch 10/10, Batch 270/1465, Loss: 1.0486\n",
      "Epoch 10/10, Batch 280/1465, Loss: 1.1593\n",
      "Epoch 10/10, Batch 290/1465, Loss: 0.8066\n",
      "Epoch 10/10, Batch 300/1465, Loss: 0.9435\n",
      "Epoch 10/10, Batch 310/1465, Loss: 1.0307\n",
      "Epoch 10/10, Batch 320/1465, Loss: 1.1823\n",
      "Epoch 10/10, Batch 330/1465, Loss: 0.9094\n",
      "Epoch 10/10, Batch 340/1465, Loss: 0.9817\n",
      "Epoch 10/10, Batch 350/1465, Loss: 1.0546\n",
      "Epoch 10/10, Batch 360/1465, Loss: 0.8906\n",
      "Epoch 10/10, Batch 370/1465, Loss: 1.0434\n",
      "Epoch 10/10, Batch 380/1465, Loss: 1.2277\n",
      "Epoch 10/10, Batch 390/1465, Loss: 0.8305\n",
      "Epoch 10/10, Batch 400/1465, Loss: 0.9797\n",
      "Epoch 10/10, Batch 410/1465, Loss: 0.9148\n",
      "Epoch 10/10, Batch 420/1465, Loss: 0.9356\n",
      "Epoch 10/10, Batch 430/1465, Loss: 1.0318\n",
      "Epoch 10/10, Batch 440/1465, Loss: 0.8530\n",
      "Epoch 10/10, Batch 450/1465, Loss: 0.9726\n",
      "Epoch 10/10, Batch 460/1465, Loss: 0.9899\n",
      "Epoch 10/10, Batch 470/1465, Loss: 0.7895\n",
      "Epoch 10/10, Batch 480/1465, Loss: 0.9290\n",
      "Epoch 10/10, Batch 490/1465, Loss: 1.1530\n",
      "Epoch 10/10, Batch 500/1465, Loss: 1.1433\n",
      "Epoch 10/10, Batch 510/1465, Loss: 1.0694\n",
      "Epoch 10/10, Batch 520/1465, Loss: 0.8448\n",
      "Epoch 10/10, Batch 530/1465, Loss: 1.0215\n",
      "Epoch 10/10, Batch 540/1465, Loss: 0.8251\n",
      "Epoch 10/10, Batch 550/1465, Loss: 0.7785\n",
      "Epoch 10/10, Batch 560/1465, Loss: 1.1250\n",
      "Epoch 10/10, Batch 570/1465, Loss: 0.9374\n",
      "Epoch 10/10, Batch 580/1465, Loss: 0.8865\n",
      "Epoch 10/10, Batch 590/1465, Loss: 1.1292\n",
      "Epoch 10/10, Batch 600/1465, Loss: 1.0595\n",
      "Epoch 10/10, Batch 610/1465, Loss: 0.9243\n",
      "Epoch 10/10, Batch 620/1465, Loss: 0.9714\n",
      "Epoch 10/10, Batch 630/1465, Loss: 0.9817\n",
      "Epoch 10/10, Batch 640/1465, Loss: 0.9032\n",
      "Epoch 10/10, Batch 650/1465, Loss: 0.8573\n",
      "Epoch 10/10, Batch 660/1465, Loss: 0.9551\n",
      "Epoch 10/10, Batch 670/1465, Loss: 0.9456\n",
      "Epoch 10/10, Batch 680/1465, Loss: 1.1123\n",
      "Epoch 10/10, Batch 690/1465, Loss: 0.8369\n",
      "Epoch 10/10, Batch 700/1465, Loss: 1.1869\n",
      "Epoch 10/10, Batch 710/1465, Loss: 0.7985\n",
      "Epoch 10/10, Batch 720/1465, Loss: 1.2636\n",
      "Epoch 10/10, Batch 730/1465, Loss: 0.9362\n",
      "Epoch 10/10, Batch 740/1465, Loss: 0.9086\n",
      "Epoch 10/10, Batch 750/1465, Loss: 1.2764\n",
      "Epoch 10/10, Batch 760/1465, Loss: 0.8587\n",
      "Epoch 10/10, Batch 770/1465, Loss: 0.8129\n",
      "Epoch 10/10, Batch 780/1465, Loss: 1.2398\n",
      "Epoch 10/10, Batch 790/1465, Loss: 0.8030\n",
      "Epoch 10/10, Batch 800/1465, Loss: 0.8724\n",
      "Epoch 10/10, Batch 810/1465, Loss: 0.7898\n",
      "Epoch 10/10, Batch 820/1465, Loss: 0.9023\n",
      "Epoch 10/10, Batch 830/1465, Loss: 0.9247\n",
      "Epoch 10/10, Batch 840/1465, Loss: 1.0887\n",
      "Epoch 10/10, Batch 850/1465, Loss: 1.3315\n",
      "Epoch 10/10, Batch 860/1465, Loss: 0.9527\n",
      "Epoch 10/10, Batch 870/1465, Loss: 0.9842\n",
      "Epoch 10/10, Batch 880/1465, Loss: 0.9859\n",
      "Epoch 10/10, Batch 890/1465, Loss: 0.8678\n",
      "Epoch 10/10, Batch 900/1465, Loss: 0.9862\n",
      "Epoch 10/10, Batch 910/1465, Loss: 0.7682\n",
      "Epoch 10/10, Batch 920/1465, Loss: 0.9299\n",
      "Epoch 10/10, Batch 930/1465, Loss: 0.8769\n",
      "Epoch 10/10, Batch 940/1465, Loss: 1.0153\n",
      "Epoch 10/10, Batch 950/1465, Loss: 1.0796\n",
      "Epoch 10/10, Batch 960/1465, Loss: 1.0471\n",
      "Epoch 10/10, Batch 970/1465, Loss: 0.7714\n",
      "Epoch 10/10, Batch 980/1465, Loss: 1.0664\n",
      "Epoch 10/10, Batch 990/1465, Loss: 0.9059\n",
      "Epoch 10/10, Batch 1000/1465, Loss: 1.0488\n",
      "Epoch 10/10, Batch 1010/1465, Loss: 1.2276\n",
      "Epoch 10/10, Batch 1020/1465, Loss: 1.0454\n",
      "Epoch 10/10, Batch 1030/1465, Loss: 1.0487\n",
      "Epoch 10/10, Batch 1040/1465, Loss: 1.0702\n",
      "Epoch 10/10, Batch 1050/1465, Loss: 1.0890\n",
      "Epoch 10/10, Batch 1060/1465, Loss: 0.6872\n",
      "Epoch 10/10, Batch 1070/1465, Loss: 0.9144\n",
      "Epoch 10/10, Batch 1080/1465, Loss: 0.9333\n",
      "Epoch 10/10, Batch 1090/1465, Loss: 0.8863\n",
      "Epoch 10/10, Batch 1100/1465, Loss: 0.7936\n",
      "Epoch 10/10, Batch 1110/1465, Loss: 0.8660\n",
      "Epoch 10/10, Batch 1120/1465, Loss: 0.8864\n",
      "Epoch 10/10, Batch 1130/1465, Loss: 1.0431\n",
      "Epoch 10/10, Batch 1140/1465, Loss: 0.9140\n",
      "Epoch 10/10, Batch 1150/1465, Loss: 0.9396\n",
      "Epoch 10/10, Batch 1160/1465, Loss: 0.8899\n",
      "Epoch 10/10, Batch 1170/1465, Loss: 1.0361\n",
      "Epoch 10/10, Batch 1180/1465, Loss: 0.8874\n",
      "Epoch 10/10, Batch 1190/1465, Loss: 0.8370\n",
      "Epoch 10/10, Batch 1200/1465, Loss: 0.9207\n",
      "Epoch 10/10, Batch 1210/1465, Loss: 1.1701\n",
      "Epoch 10/10, Batch 1220/1465, Loss: 0.8092\n",
      "Epoch 10/10, Batch 1230/1465, Loss: 0.9792\n",
      "Epoch 10/10, Batch 1240/1465, Loss: 1.0440\n",
      "Epoch 10/10, Batch 1250/1465, Loss: 0.9499\n",
      "Epoch 10/10, Batch 1260/1465, Loss: 0.9928\n",
      "Epoch 10/10, Batch 1270/1465, Loss: 1.0132\n",
      "Epoch 10/10, Batch 1280/1465, Loss: 0.9949\n",
      "Epoch 10/10, Batch 1290/1465, Loss: 1.1835\n",
      "Epoch 10/10, Batch 1300/1465, Loss: 0.6342\n",
      "Epoch 10/10, Batch 1310/1465, Loss: 1.1282\n",
      "Epoch 10/10, Batch 1320/1465, Loss: 0.9994\n",
      "Epoch 10/10, Batch 1330/1465, Loss: 0.6008\n",
      "Epoch 10/10, Batch 1340/1465, Loss: 0.9461\n",
      "Epoch 10/10, Batch 1350/1465, Loss: 0.8752\n",
      "Epoch 10/10, Batch 1360/1465, Loss: 1.2383\n",
      "Epoch 10/10, Batch 1370/1465, Loss: 1.0768\n",
      "Epoch 10/10, Batch 1380/1465, Loss: 0.9479\n",
      "Epoch 10/10, Batch 1390/1465, Loss: 0.7238\n",
      "Epoch 10/10, Batch 1400/1465, Loss: 0.9779\n",
      "Epoch 10/10, Batch 1410/1465, Loss: 1.0671\n",
      "Epoch 10/10, Batch 1420/1465, Loss: 0.9193\n",
      "Epoch 10/10, Batch 1430/1465, Loss: 1.0996\n",
      "Epoch 10/10, Batch 1440/1465, Loss: 1.1219\n",
      "Epoch 10/10, Batch 1450/1465, Loss: 1.0555\n",
      "Epoch 10/10, Batch 1460/1465, Loss: 0.9760\n",
      "Epoch [10/10], Average Loss: 0.9694\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "reviews = df['content'].tolist()  # 'content'를 리스트로 변환\n",
    "ratings = df['score'].tolist()    # 'score'를 리스트로 변환\n",
    "\n",
    "\n",
    "\n",
    "# 라벨을 정수형으로 변환 (필수적인 과정)\n",
    "label_encoder = LabelEncoder()\n",
    "ratings = label_encoder.fit_transform(ratings)  # 평점 정수형으로 변환\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, text_pipeline, label_pipeline):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.text_pipeline(self.reviews[idx])\n",
    "        rating = self.label_pipeline(self.ratings[idx])\n",
    "        return torch.tensor(review), torch.tensor(rating)\n",
    "\n",
    "# 토크나이저 정의 (기본 영어 토크나이저)\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 어휘 사전 생성 함수\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# 어휘 사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(reviews))\n",
    "\n",
    "\n",
    "# 텍스트 파이프라인 정의 (어휘 사전에 있는 단어만 처리)\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "# 평점 그대로 사용\n",
    "def label_pipeline(label):\n",
    "    return label  # 이미 숫자형이므로 변환 생략\n",
    "\n",
    "# 데이터를 학습용(train)과 테스트용(test)으로 분리\n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋 정의\n",
    "train_dataset = ReviewDataset(train_reviews, train_ratings, text_pipeline, label_pipeline)\n",
    "test_dataset = ReviewDataset(test_reviews, test_ratings, text_pipeline, label_pipeline)\n",
    "\n",
    "# 패딩을 적용하는 함수 정의\n",
    "\n",
    "def collate_fn(batch):\n",
    "    reviews, ratings = zip(*batch)\n",
    "    reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n",
    "    ratings = torch.tensor(ratings, dtype=torch.long)  # 평점도 정수형으로 변환\n",
    "    return reviews, ratings\n",
    "# 데이터 로더 정의\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding으로 변경\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(set(ratings))  # 예측할 점수 개수 (평점이 정수형)\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # SGD 에서 Adam으로 변경\n",
    "\n",
    "# 모델을 CUDA로 이동 (가능한 경우)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 에포크마다 손실을 추적\n",
    "        for i, (reviews, ratings) in enumerate(train_dataloader):\n",
    "            reviews, ratings = reviews.to(device), ratings.to(device)  # 데이터를 GPU로 이동\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(reviews)  # 모델에 입력하여 예측값 계산\n",
    "            loss = criterion(outputs, ratings)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 배치마다 손실 출력\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss/len(train_dataloader):.4f}')\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72062d5b-dbd3-4936-becd-fa5302d76fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_17888\\2771336819.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.35113330772186%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않음\n",
    "    for reviews, ratings in test_dataloader:\n",
    "        reviews, ratings = reviews.to(device), ratings.to(device)\n",
    "        outputs = model(reviews)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += ratings.size(0)\n",
    "        correct += (predicted == ratings).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b984ce-e3bb-403b-91e3-71162645e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)  # 모델 파라미터가 GPU에 있는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ded08-6e57-4ed6-a192-77e138025ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
