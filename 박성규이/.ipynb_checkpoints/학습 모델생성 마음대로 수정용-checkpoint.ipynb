{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6baa5e-cda8-4638-9325-a8e54a45f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117134lines [00:01, 86041.31lines/s] \n",
      ".vector_cache\\glove.6B.zip: 862MB [02:48, 5.12MB/s]                                                                    \n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:13<00:00, 29847.38it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'module' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m glove \u001b[38;5;241m=\u001b[39m vocab\u001b[38;5;241m.\u001b[39mGloVe(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6B\u001b[39m\u001b[38;5;124m'\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# 100차원의 GloVe 임베딩 사용\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# 임베딩 레이어에 GloVe 적용\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab):\n\u001b[0;32m    154\u001b[0m     embedding_matrix[i] \u001b[38;5;241m=\u001b[39m glove[word]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'module' has no len()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"netflix_reviews.csv\")  # 파일 불러오기\n",
    "df = df.iloc[:,0:5]\n",
    "\n",
    "# 전처리 함수\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "# 이모티콘만 추출하는 함수 (중복 제거)\n",
    "def remove_duplicate_emojis(text):\n",
    "    # 유니코드 이모티콘 범위에 해당하는 모든 이모티콘을 찾음\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F]\", flags=re.UNICODE)\n",
    "    \n",
    "    # 중복 제거를 위한 세트 (set) 사용\n",
    "    emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # 텍스트에서 중복된 이모티콘을 제거하고, 하나의 이모티콘만 남김\n",
    "    for em in emojis:\n",
    "        text = re.sub(em + '+', em, text)  # 중복된 이모티콘을 하나로 줄임\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 전처리 함수 (이모티콘 중복 제거 후 텍스트로 변환)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    \n",
    "    # 이모티콘 중복 제거\n",
    "    text = remove_duplicate_emojis(text)\n",
    "    \n",
    "    # 이모티콘을 텍스트로 변환\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # 소문자로 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 숫자 및 구두점 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df['reviewId'] = df['reviewId'].apply(preprocess_text)\n",
    "df['userName'] = df['userName'].apply(preprocess_text)\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "#\n",
    "reviews = df['content'].tolist()  # 'content'를 리스트로 변환\n",
    "ratings = df['score'].tolist()    # 'score'를 리스트로 변환\n",
    "\n",
    "\n",
    "\n",
    "# 라벨을 정수형으로 변환 (필수적인 과정)\n",
    "label_encoder = LabelEncoder()\n",
    "ratings = label_encoder.fit_transform(ratings)  # 평점 정수형으로 변환\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, text_pipeline, label_pipeline):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.text_pipeline = text_pipeline\n",
    "        self.label_pipeline = label_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.text_pipeline(self.reviews[idx])\n",
    "        rating = self.label_pipeline(self.ratings[idx])\n",
    "        return torch.tensor(review), torch.tensor(rating)\n",
    "\n",
    "# 토크나이저 정의 (기본 영어 토크나이저)\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 어휘 사전 생성 함수\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# 어휘 사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(reviews))\n",
    "\n",
    "\n",
    "# 텍스트 파이프라인 정의 (어휘 사전에 있는 단어만 처리)\n",
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "# 평점 그대로 사용\n",
    "def label_pipeline(label):\n",
    "    return label  # 이미 숫자형이므로 변환 생략\n",
    "\n",
    "# 데이터를 학습용(train)과 테스트용(test)으로 분리\n",
    "train_reviews, test_reviews, train_ratings, test_ratings = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋 정의\n",
    "train_dataset = ReviewDataset(train_reviews, train_ratings, text_pipeline, label_pipeline)\n",
    "test_dataset = ReviewDataset(test_reviews, test_ratings, text_pipeline, label_pipeline)\n",
    "\n",
    "# 패딩을 적용하는 함수 정의\n",
    "\n",
    "def collate_fn(batch):\n",
    "    reviews, ratings = zip(*batch)\n",
    "    reviews = pad_sequence([torch.tensor(r, dtype=torch.long) for r in reviews], batch_first=True)  # 정수형 텐서로 변환\n",
    "    ratings = torch.tensor(ratings, dtype=torch.long)  # 평점도 정수형으로 변환\n",
    "    return reviews, ratings\n",
    "# 데이터 로더 정의\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding으로 변경\n",
    "        # self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.5) # layer 2개 , 드롭아웃 적용(과적합 방지)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(set(ratings))  # 예측할 점수 개수 (평점이 정수형)\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # SGD 에서 Adam으로 변경 lr : 0.01 - > 0.001 / Accuracy: 63% -> 61.59% 다시 \n",
    "\n",
    "# 모델을 CUDA로 이동 (가능한 경우)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()  # 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 에포크마다 손실을 추적\n",
    "        for i, (reviews, ratings) in enumerate(train_dataloader):\n",
    "            reviews, ratings = reviews.to(device), ratings.to(device)  # 데이터를 GPU로 이동\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(reviews)  # 모델에 입력하여 예측값 계산\n",
    "            loss = criterion(outputs, ratings)  # 손실 계산\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 배치마다 손실 출력\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss/len(train_dataloader):.4f}')\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않음\n",
    "    for reviews, ratings in test_dataloader:\n",
    "        reviews, ratings = reviews.to(device), ratings.to(device)\n",
    "        outputs = model(reviews)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += ratings.size(0)\n",
    "        correct += (predicted == ratings).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "\n",
    "\n",
    "#수정 \n",
    "# self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "# self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
